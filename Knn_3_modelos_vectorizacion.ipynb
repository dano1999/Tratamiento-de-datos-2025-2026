{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df5ae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en label (antes de limpiar):\n",
      "  train: 2\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "NaN en label (después de limpiar):\n",
      "  train: 0\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "Etiquetas únicas en train: ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Distribución de clases:\n",
      "\n",
      " TRAIN (total = 4877) \n",
      "comment : 3495 (0.717)\n",
      "support :  642 (0.132)\n",
      "query   :  373 (0.076)\n",
      "deny    :  367 (0.075)\n",
      "\n",
      " VAL (total = 1440) \n",
      "comment : 1174 (0.815)\n",
      "query   :  114 (0.079)\n",
      "deny    :   79 (0.055)\n",
      "support :   73 (0.051)\n",
      "\n",
      " TEST (total = 1675) \n",
      "comment : 1405 (0.839)\n",
      "support :  104 (0.062)\n",
      "deny    :  100 (0.060)\n",
      "query   :   66 (0.039)\n",
      "\n",
      "Ejemplo de texto de entrenamiento:\n",
      "France: 10 people dead after shooting at HQ of satirical weekly newspaper #CharlieHebdo, according to witnesses http://t.co/FkYxGmuS58 [SEP] MT @euronews France: 10 dead after shooting at HQ of satirical weekly #CharlieHebdo. If Zionists/Jews did this they'd be nuking Israel\n",
      "Etiqueta: comment\n",
      "\n",
      "Clases (label_encoder): ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Clase mayoritaria en TEST: comment\n",
      "Accuracy baseline (siempre 'comment') = 0.8388\n",
      "\n",
      "\n",
      "TF-IDF + KNN\n",
      "RESULTADOS KNN - TF-IDF\n",
      "k = 1 --> Accuracy validación = 0.6632\n",
      "k = 3 --> Accuracy validación = 0.7118\n",
      "k = 5 --> Accuracy validación = 0.7389\n",
      "k = 7 --> Accuracy validación = 0.7681\n",
      "k = 9 --> Accuracy validación = 0.7903\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.7903\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8299\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8412    0.9879    0.9087      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.1000    0.0192    0.0323       104\n",
      "\n",
      "    accuracy                         0.8299      1675\n",
      "   macro avg     0.2353    0.2518    0.2352      1675\n",
      "weighted avg     0.7118    0.8299    0.7642      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'support' 'comment' 'comment' 'comment' 'support' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'support' 'support' 'comment'\n",
      " 'comment' 'comment' 'support' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\n",
      "Texto 0:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 1:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 2:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 3:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 4:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 5:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 6:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 7:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 8:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 9:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "\n",
      "\n",
      "TF-IDF + CNN\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - TF-IDF + CNN\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n",
      "Época 01/15 | Loss train = 5.2414 | Acc train = 0.2612 | Acc val = 0.0549\n",
      "Época 02/15 | Loss train = 2.7086 | Acc train = 0.2688 | Acc val = 0.5826\n",
      "Época 03/15 | Loss train = 2.2786 | Acc train = 0.2887 | Acc val = 0.6368\n",
      "Época 04/15 | Loss train = 1.9276 | Acc train = 0.2614 | Acc val = 0.1556\n",
      "Época 05/15 | Loss train = 1.6589 | Acc train = 0.2526 | Acc val = 0.6896\n",
      "Época 06/15 | Loss train = 1.5291 | Acc train = 0.2415 | Acc val = 0.8035\n",
      "Época 07/15 | Loss train = 1.4457 | Acc train = 0.2867 | Acc val = 0.0597\n",
      "Época 08/15 | Loss train = 1.4220 | Acc train = 0.2801 | Acc val = 0.7035\n",
      "Época 09/15 | Loss train = 1.4058 | Acc train = 0.2711 | Acc val = 0.7944\n",
      "Época 10/15 | Loss train = 1.3994 | Acc train = 0.2973 | Acc val = 0.7986\n",
      "Época 11/15 | Loss train = 1.4072 | Acc train = 0.2563 | Acc val = 0.8153\n",
      "Época 12/15 | Loss train = 1.3958 | Acc train = 0.3611 | Acc val = 0.0549\n",
      "Época 13/15 | Loss train = 1.3956 | Acc train = 0.2985 | Acc val = 0.8153\n",
      "Época 14/15 | Loss train = 1.3954 | Acc train = 0.3322 | Acc val = 0.1340\n",
      "Época 15/15 | Loss train = 1.3957 | Acc train = 0.3170 | Acc val = 0.6722\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.8153\n",
      "\n",
      "Accuracy en TEST (TF-IDF + CNN) = 0.8000\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8384    0.9488    0.8902      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0843    0.0673    0.0749       104\n",
      "\n",
      "    accuracy                         0.8000      1675\n",
      "   macro avg     0.2307    0.2540    0.2413      1675\n",
      "weighted avg     0.7085    0.8000    0.7513      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN TF-IDF + CNN (primeras 10 líneas)\n",
      "Texto 0:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 1:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 2:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 3:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 4:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 5:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 6:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 7:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 8:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 9:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "\n",
      "\n",
      "Word2Vec + KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS KNN - Word2Vec (media embeddings)\n",
      "k = 1 --> Accuracy validación = 0.6785\n",
      "k = 3 --> Accuracy validación = 0.7764\n",
      "k = 5 --> Accuracy validación = 0.7965\n",
      "k = 7 --> Accuracy validación = 0.8083\n",
      "k = 9 --> Accuracy validación = 0.8125\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.8125\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8376\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8386    0.9986    0.9116      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8376      1675\n",
      "   macro avg     0.2097    0.2496    0.2279      1675\n",
      "weighted avg     0.7034    0.8376    0.7647      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN Word2Vec + KNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "Word2Vec + CNN\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - Word2Vec + CNN\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 01/30 | Loss train = 1.5797 | Acc train = 0.2461 | Acc val = 0.2243\n",
      "Época 02/30 | Loss train = 1.4152 | Acc train = 0.2992 | Acc val = 0.2660\n",
      "Época 03/30 | Loss train = 1.3798 | Acc train = 0.3160 | Acc val = 0.7368\n",
      "Época 04/30 | Loss train = 1.3485 | Acc train = 0.3728 | Acc val = 0.5653\n",
      "Época 05/30 | Loss train = 1.3411 | Acc train = 0.3469 | Acc val = 0.6681\n",
      "Época 06/30 | Loss train = 1.3214 | Acc train = 0.3576 | Acc val = 0.7667\n",
      "Época 07/30 | Loss train = 1.3198 | Acc train = 0.3637 | Acc val = 0.6840\n",
      "Época 08/30 | Loss train = 1.3084 | Acc train = 0.3580 | Acc val = 0.2514\n",
      "Época 09/30 | Loss train = 1.2974 | Acc train = 0.3719 | Acc val = 0.7819\n",
      "Época 10/30 | Loss train = 1.2967 | Acc train = 0.3601 | Acc val = 0.3604\n",
      "Época 11/30 | Loss train = 1.3018 | Acc train = 0.3601 | Acc val = 0.5618\n",
      "Época 12/30 | Loss train = 1.2839 | Acc train = 0.3662 | Acc val = 0.0896\n",
      "Época 13/30 | Loss train = 1.2730 | Acc train = 0.3707 | Acc val = 0.6715\n",
      "Época 14/30 | Loss train = 1.2609 | Acc train = 0.3732 | Acc val = 0.3326\n",
      "Época 15/30 | Loss train = 1.2562 | Acc train = 0.3826 | Acc val = 0.4667\n",
      "Época 16/30 | Loss train = 1.2465 | Acc train = 0.3691 | Acc val = 0.3361\n",
      "Época 17/30 | Loss train = 1.2437 | Acc train = 0.3816 | Acc val = 0.6792\n",
      "Época 18/30 | Loss train = 1.2307 | Acc train = 0.3810 | Acc val = 0.5194\n",
      "Época 19/30 | Loss train = 1.2280 | Acc train = 0.3851 | Acc val = 0.2951\n",
      "Época 20/30 | Loss train = 1.2371 | Acc train = 0.3773 | Acc val = 0.3569\n",
      "Época 21/30 | Loss train = 1.2356 | Acc train = 0.4027 | Acc val = 0.1938\n",
      "Época 22/30 | Loss train = 1.2216 | Acc train = 0.3988 | Acc val = 0.7493\n",
      "Época 23/30 | Loss train = 1.2005 | Acc train = 0.4076 | Acc val = 0.6562\n",
      "Época 24/30 | Loss train = 1.1842 | Acc train = 0.4181 | Acc val = 0.4896\n",
      "Época 25/30 | Loss train = 1.1935 | Acc train = 0.4015 | Acc val = 0.5861\n",
      "Época 26/30 | Loss train = 1.2029 | Acc train = 0.4091 | Acc val = 0.5924\n",
      "Época 27/30 | Loss train = 1.1653 | Acc train = 0.4228 | Acc val = 0.5236\n",
      "Época 28/30 | Loss train = 1.1704 | Acc train = 0.4298 | Acc val = 0.5813\n",
      "Época 29/30 | Loss train = 1.1470 | Acc train = 0.4261 | Acc val = 0.5986\n",
      "Época 30/30 | Loss train = 1.1482 | Acc train = 0.4199 | Acc val = 0.5924\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.7819\n",
      "\n",
      "Accuracy en TEST (Word2Vec + CNN) = 0.5528\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8423    0.6384    0.7263      1405\n",
      "        deny     0.0492    0.2100    0.0797       100\n",
      "       query     0.0444    0.1212    0.0650        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.5528      1675\n",
      "   macro avg     0.2340    0.2424    0.2178      1675\n",
      "weighted avg     0.7112    0.5528    0.6166      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'support' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'deny' 'comment' 'comment' 'deny'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN Word2Vec + CNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=support  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "EMBEDDINGS (Sentence-BERT) + KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 153/153 [00:01<00:00, 110.61it/s]\n",
      "Batches: 100%|██████████| 45/45 [00:00<00:00, 135.65it/s]\n",
      "Batches: 100%|██████████| 53/53 [00:00<00:00, 129.76it/s]\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS KNN - Embeddings contextuales (Sentence-BERT)\n",
      "k = 1 --> Accuracy validación = 0.6465\n",
      "k = 3 --> Accuracy validación = 0.6931\n",
      "k = 5 --> Accuracy validación = 0.7701\n",
      "k = 7 --> Accuracy validación = 0.7868\n",
      "k = 9 --> Accuracy validación = 0.8035\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.8035\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8304\n",
      "\n",
      "Classification report (TEST):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8395    0.9900    0.9086      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8304      1675\n",
      "   macro avg     0.2099    0.2475    0.2271      1675\n",
      "weighted avg     0.7042    0.8304    0.7621      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN BERT + KNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "BERT Embeddings + CNN \n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - Sentence-BERT + CNN\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n",
      "Época 01/30 | Loss train = 1.7402 | Acc train = 0.2680 | Acc val = 0.2514\n",
      "Época 02/30 | Loss train = 1.5176 | Acc train = 0.2916 | Acc val = 0.6715\n",
      "Época 03/30 | Loss train = 1.4548 | Acc train = 0.3125 | Acc val = 0.5556\n",
      "Época 04/30 | Loss train = 1.4264 | Acc train = 0.3106 | Acc val = 0.4486\n",
      "Época 05/30 | Loss train = 1.4120 | Acc train = 0.3031 | Acc val = 0.6271\n",
      "Época 06/30 | Loss train = 1.3990 | Acc train = 0.3180 | Acc val = 0.7931\n",
      "Época 07/30 | Loss train = 1.3980 | Acc train = 0.3291 | Acc val = 0.5910\n",
      "Época 08/30 | Loss train = 1.3788 | Acc train = 0.3348 | Acc val = 0.6021\n",
      "Época 09/30 | Loss train = 1.3852 | Acc train = 0.3402 | Acc val = 0.6396\n",
      "Época 10/30 | Loss train = 1.3693 | Acc train = 0.3270 | Acc val = 0.6965\n",
      "Época 11/30 | Loss train = 1.3715 | Acc train = 0.3517 | Acc val = 0.2326\n",
      "Época 12/30 | Loss train = 1.3619 | Acc train = 0.3492 | Acc val = 0.6479\n",
      "Época 13/30 | Loss train = 1.3576 | Acc train = 0.3469 | Acc val = 0.6312\n",
      "Época 14/30 | Loss train = 1.3490 | Acc train = 0.3482 | Acc val = 0.1167\n",
      "Época 15/30 | Loss train = 1.3374 | Acc train = 0.3473 | Acc val = 0.4333\n",
      "Época 16/30 | Loss train = 1.3439 | Acc train = 0.3484 | Acc val = 0.6806\n",
      "Época 17/30 | Loss train = 1.3365 | Acc train = 0.3539 | Acc val = 0.7056\n",
      "Época 18/30 | Loss train = 1.3187 | Acc train = 0.3555 | Acc val = 0.1764\n",
      "Época 19/30 | Loss train = 1.3180 | Acc train = 0.3260 | Acc val = 0.6292\n",
      "Época 20/30 | Loss train = 1.3106 | Acc train = 0.3443 | Acc val = 0.6347\n",
      "Época 21/30 | Loss train = 1.3122 | Acc train = 0.3285 | Acc val = 0.6556\n",
      "Época 22/30 | Loss train = 1.3117 | Acc train = 0.3605 | Acc val = 0.3736\n",
      "Época 23/30 | Loss train = 1.3006 | Acc train = 0.3623 | Acc val = 0.7007\n",
      "Época 24/30 | Loss train = 1.2746 | Acc train = 0.3658 | Acc val = 0.4368\n",
      "Época 25/30 | Loss train = 1.2741 | Acc train = 0.3564 | Acc val = 0.4986\n",
      "Época 26/30 | Loss train = 1.2387 | Acc train = 0.3678 | Acc val = 0.6979\n",
      "Época 27/30 | Loss train = 1.2602 | Acc train = 0.3644 | Acc val = 0.3417\n",
      "Época 28/30 | Loss train = 1.2262 | Acc train = 0.3590 | Acc val = 0.7910\n",
      "Época 29/30 | Loss train = 1.2228 | Acc train = 0.3740 | Acc val = 0.5354\n",
      "Época 30/30 | Loss train = 1.2113 | Acc train = 0.3627 | Acc val = 0.8049\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.8049\n",
      "\n",
      "Accuracy en TEST (Sentence-BERT + CNN) = 0.8376\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8391    0.9986    0.9119      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8376      1675\n",
      "   macro avg     0.2098    0.2496    0.2280      1675\n",
      "weighted avg     0.7039    0.8376    0.7649      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'query' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN BERT + CNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "TRANSFORMER PREENTRENADO + FINE-TUNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo para Transformer: cuda\n",
      "[Transformer] Época 1/3 | Loss train = 0.8633 | Acc train = 0.7136 | Acc val = 0.8403\n",
      "[Transformer] Época 2/3 | Loss train = 0.7328 | Acc train = 0.7460 | Acc val = 0.8208\n",
      "[Transformer] Época 3/3 | Loss train = 0.6124 | Acc train = 0.7855 | Acc val = 0.8146\n",
      "\n",
      "Mejor accuracy de validación (Transformer) = 0.8403\n",
      "\n",
      "Accuracy en TEST (Transformer fine-tuned: distilbert-base-uncased) = 0.8316\n",
      "\n",
      "Classification report (TEST) - Transformer fine-tuned:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8527    0.9680    0.9067      1405\n",
      "        deny     0.4348    0.1000    0.1626       100\n",
      "       query     0.4167    0.3030    0.3509        66\n",
      "     support     0.3333    0.0288    0.0531       104\n",
      "\n",
      "    accuracy                         0.8316      1675\n",
      "   macro avg     0.5094    0.3500    0.3683      1675\n",
      "weighted avg     0.7783    0.8316    0.7873      1675\n",
      "\n",
      "\n",
      "\n",
      "RESUMEN FINAL - KNN\n",
      "TF-IDF (KNN):        mejor k = 9,  accuracy test = 0.8299\n",
      "Word2Vec (KNN):      mejor k = 9,    accuracy test = 0.8376\n",
      "Sentence-BERT (KNN): mejor k = 9,   accuracy test = 0.8304\n",
      "\n",
      "RESUMEN FINAL - CNN\n",
      "TF-IDF  + CNN:        accuracy test = 0.8000\n",
      "Word2Vec + CNN:       accuracy test = 0.5528\n",
      "Sentence-BERT + CNN:  accuracy test = 0.8376\n",
      "\n",
      "Baseline mayoría ('comment') en TEST: accuracy = 0.8388\n",
      "\n",
      "RESUMEN FINAL - TRANSFORMER FINE-TUNED\n",
      "Transformer (distilbert-base-uncased): accuracy test = 0.8316\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "train_path = \"data/datasets/rumoureval2019_train.csv\"\n",
    "val_path   = \"data/datasets/rumoureval2019_val.csv\"\n",
    "test_path  = \"data/datasets/rumoureval2019_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df   = pd.read_csv(val_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"NaN en label (antes de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"label\"])\n",
    "val_df   = val_df.dropna(subset=[\"label\"])\n",
    "test_df  = test_df.dropna(subset=[\"label\"])\n",
    "\n",
    "print(\"\\nNaN en label (después de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "print(\"\\nEtiquetas únicas en train:\", train_df[\"label\"].unique())\n",
    "\n",
    "print(\"\\nDistribución de clases:\")\n",
    "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    counts = df[\"label\"].value_counts()\n",
    "    total = counts.sum()\n",
    "    print(f\"\\n {name.upper()} (total = {total}) \")\n",
    "    for label, c in counts.items():\n",
    "        print(f\"{label:8s}: {c:4d} ({c/total:.3f})\")\n",
    "\n",
    "def concat_text_row(row):\n",
    "    src = row.get(\"source_text\", \"\")\n",
    "    rep = row.get(\"reply_text\", \"\")\n",
    "    src = \"\" if pd.isna(src) else str(src)\n",
    "    rep = \"\" if pd.isna(rep) else str(rep)\n",
    "    return (src + \" [SEP] \" + rep).strip()\n",
    "\n",
    "X_train_text = train_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_train = train_df[\"label\"].values         \n",
    "\n",
    "X_val_text = val_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "X_test_text = test_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\"\\nEjemplo de texto de entrenamiento:\")\n",
    "print(X_train_text[0])\n",
    "print(\"Etiqueta:\", y_train[0])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_idx = label_encoder.fit_transform(y_train)\n",
    "y_val_idx   = label_encoder.transform(y_val)\n",
    "y_test_idx  = label_encoder.transform(y_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nClases (label_encoder):\", label_encoder.classes_)\n",
    "\n",
    "major_class = Counter(y_test).most_common(1)[0][0]\n",
    "baseline_acc = np.mean(y_test == major_class)\n",
    "print(f\"\\nClase mayoritaria en TEST: {major_class}\")\n",
    "print(f\"Accuracy baseline (siempre '{major_class}') = {baseline_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate_knn(X_train_vec, y_train,\n",
    "                           X_val_vec, y_val,\n",
    "                           X_test_vec, y_test,\n",
    "                           k_values=[1, 3, 5, 7, 9],\n",
    "                           title=\"\"):\n",
    "    print(\"RESULTADOS KNN -\", title)\n",
    "\n",
    "    best_k = None\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_vec, y_train)\n",
    "        y_val_pred = knn.predict(X_val_vec)\n",
    "        acc_val = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"k = {k} --> Accuracy validación = {acc_val:.4f}\")\n",
    "\n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            best_k = k\n",
    "\n",
    "    print(\"\\nMejor número de vecinos (k) encontrado en validación:\", best_k)\n",
    "    print(f\"Accuracy de validación con k={best_k}: {best_acc:.4f}\")\n",
    "\n",
    "    final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    final_knn.fit(X_train_vec, y_train)\n",
    "\n",
    "    y_test_pred = final_knn.predict(X_test_vec)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"\\nAccuracy en TEST con k={best_k}: {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred[:20])\n",
    "    print(\"y_test[:20]      =\", y_test[:20])\n",
    "\n",
    "    return final_knn, best_k, acc_test\n",
    "\n",
    "\n",
    "class ConvNet1D(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, dropout=0.3):\n",
    "        super(ConvNet1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,   out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn4   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)           # (B, 1, L)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # (B, 64, L)\n",
    "\n",
    "        x = self.global_pool(x)      # (B, 64, 1)\n",
    "        x = x.squeeze(-1)            # (B, 64)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)               # (B, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_cnn(\n",
    "    X_train, y_train_idx,\n",
    "    X_val, y_val_idx,\n",
    "    X_test, y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"CNN\",\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3,\n",
    "    device=None\n",
    "):\n",
    "\n",
    "    print(\"ENTRENANDO RED NEURONAL CONVOLUCIONAL -\", title)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    X_val   = np.asarray(X_val,   dtype=np.float32)\n",
    "    X_test  = np.asarray(X_test,  dtype=np.float32)\n",
    "\n",
    "    y_train_idx = np.asarray(y_train_idx, dtype=np.int64)\n",
    "    y_val_idx   = np.asarray(y_val_idx,   dtype=np.int64)\n",
    "    y_test_idx  = np.asarray(y_test_idx,  dtype=np.int64)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_idx))\n",
    "    val_dataset   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val_idx))\n",
    "    test_dataset  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    class_weights_np = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.arange(num_classes),\n",
    "        y=y_train_idx\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
    "    print(\"\\nPesos de clase (para CrossEntropyLoss):\")\n",
    "    for idx, w in enumerate(class_weights_np):\n",
    "        print(f\"  Clase {idx} ({label_encoder.classes_[idx]}): {w:.4f}\")\n",
    "\n",
    "    model = ConvNet1D(input_dim=input_dim, num_classes=num_classes, dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == batch_y).sum().item()\n",
    "            total_train += batch_X.size(0)\n",
    "\n",
    "        train_loss = running_loss / total_train\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_X.size(0)\n",
    "\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        print(f\"Época {epoch:02d}/{num_epochs} | \"\n",
    "              f\"Loss train = {train_loss:.4f} | \"\n",
    "              f\"Acc train = {train_acc:.4f} | \"\n",
    "              f\"Acc val = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "    print(f\"\\nMejor accuracy de validación alcanzado: {best_val_acc:.4f}\")\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.inference_mode():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(batch_y.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_true  = np.concatenate(all_true)\n",
    "\n",
    "    y_test_pred_labels = label_encoder.inverse_transform(all_preds)\n",
    "    y_test_true_labels = label_encoder.inverse_transform(all_true)\n",
    "\n",
    "    acc_test = accuracy_score(y_test_true_labels, y_test_pred_labels)\n",
    "    print(f\"\\nAccuracy en TEST ({title}) = {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test_true_labels, y_test_pred_labels, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred_labels[:20])\n",
    "    print(\"y_test[:20]      =\", y_test_true_labels[:20])\n",
    "\n",
    "    return model, acc_test, y_test_pred_labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + KNN\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_val_tfidf   = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "knn_tfidf, best_k_tfidf, acc_test_tfidf = train_and_evaluate_knn(\n",
    "    X_train_tfidf, y_train,\n",
    "    X_val_tfidf, y_val,\n",
    "    X_test_tfidf, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"TF-IDF\"\n",
    ")\n",
    "\n",
    "y_pred_test = knn_tfidf.predict(X_test_tfidf)\n",
    "print(\"\\nPREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + CNN\")\n",
    "\n",
    "# Normalización con StandardScaler (with_mean=False porque es sparse)\n",
    "scaler_tfidf = StandardScaler(with_mean=False)\n",
    "X_train_tfidf_scaled = scaler_tfidf.fit_transform(X_train_tfidf)\n",
    "X_val_tfidf_scaled   = scaler_tfidf.transform(X_val_tfidf)\n",
    "X_test_tfidf_scaled  = scaler_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "# Pasamos a denso para PyTorch\n",
    "X_train_tfidf_dense = X_train_tfidf_scaled.toarray()\n",
    "X_val_tfidf_dense   = X_val_tfidf_scaled.toarray()\n",
    "X_test_tfidf_dense  = X_test_tfidf_scaled.toarray()\n",
    "\n",
    "cnn_tfidf, acc_test_tfidf_cnn, y_pred_test_tfidf_cnn = train_and_evaluate_cnn(\n",
    "    X_train_tfidf_dense, y_train_idx,\n",
    "    X_val_tfidf_dense,   y_val_idx,\n",
    "    X_test_tfidf_dense,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"TF-IDF + CNN\",\n",
    "    num_epochs=15,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN TF-IDF + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test_tfidf_cnn[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + KNN\")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "train_tokens = [simple_tokenize(t) for t in X_train_text]\n",
    "val_tokens   = [simple_tokenize(t) for t in X_val_text]\n",
    "test_tokens  = [simple_tokenize(t) for t in X_test_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "def document_embedding(tokens, word_vectors, dim=100):\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        if tok in word_vectors:\n",
    "            vecs.append(word_vectors[tok])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    else:\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "def build_doc_matrix(list_of_tokens, word_vectors, dim=100):\n",
    "    return np.vstack([\n",
    "        document_embedding(toks, word_vectors, dim)\n",
    "        for toks in list_of_tokens\n",
    "    ])\n",
    "\n",
    "X_train_w2v = build_doc_matrix(train_tokens, word_vectors, dim=100)\n",
    "X_val_w2v   = build_doc_matrix(val_tokens,   word_vectors, dim=100)\n",
    "X_test_w2v  = build_doc_matrix(test_tokens,  word_vectors, dim=100)\n",
    "\n",
    "knn_w2v, best_k_w2v, acc_test_w2v = train_and_evaluate_knn(\n",
    "    X_train_w2v, y_train,\n",
    "    X_val_w2v, y_val,\n",
    "    X_test_w2v, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Word2Vec (media embeddings)\"\n",
    ")\n",
    "\n",
    "y_pred_test_w2v = knn_w2v.predict(X_test_w2v)\n",
    "print(\"\\nPREDICCIÓN Word2Vec + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + CNN\")\n",
    "\n",
    "scaler_w2v = StandardScaler()\n",
    "X_train_w2v_scaled = scaler_w2v.fit_transform(X_train_w2v)\n",
    "X_val_w2v_scaled   = scaler_w2v.transform(X_val_w2v)\n",
    "X_test_w2v_scaled  = scaler_w2v.transform(X_test_w2v)\n",
    "\n",
    "cnn_w2v, acc_test_w2v_cnn, y_pred_test_w2v_cnn = train_and_evaluate_cnn(\n",
    "    X_train_w2v_scaled, y_train_idx,\n",
    "    X_val_w2v_scaled,   y_val_idx,\n",
    "    X_test_w2v_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Word2Vec + CNN\",\n",
    "    num_epochs=30,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN Word2Vec + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nEMBEDDINGS (Sentence-BERT) + KNN\")\n",
    "\n",
    "bert_model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "X_train_bert = bert_model_st.encode(X_train_text, batch_size=32, show_progress_bar=True)\n",
    "X_val_bert   = bert_model_st.encode(X_val_text,   batch_size=32, show_progress_bar=True)\n",
    "X_test_bert  = bert_model_st.encode(X_test_text,  batch_size=32, show_progress_bar=True)\n",
    "\n",
    "knn_bert, best_k_bert, acc_test_bert = train_and_evaluate_knn(\n",
    "    X_train_bert, y_train,\n",
    "    X_val_bert,   y_val,\n",
    "    X_test_bert,  y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Embeddings contextuales (Sentence-BERT)\"\n",
    ")\n",
    "\n",
    "y_pred_test_bert = knn_bert.predict(X_test_bert)\n",
    "print(\"\\nPREDICCIÓN BERT + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert[i]}  real={y_test[i]}\")\n",
    "\n",
    "print(\"\\n\\nBERT Embeddings + CNN \")\n",
    "\n",
    "scaler_bert = StandardScaler()\n",
    "X_train_bert_scaled = scaler_bert.fit_transform(X_train_bert)\n",
    "X_val_bert_scaled   = scaler_bert.transform(X_val_bert)\n",
    "X_test_bert_scaled  = scaler_bert.transform(X_test_bert)\n",
    "\n",
    "cnn_bert, acc_test_bert_cnn, y_pred_test_bert_cnn = train_and_evaluate_cnn(\n",
    "    X_train_bert_scaled, y_train_idx,\n",
    "    X_val_bert_scaled,   y_val_idx,\n",
    "    X_test_bert_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Sentence-BERT + CNN\",\n",
    "    num_epochs=30,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN BERT + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nTRANSFORMER PREENTRENADO + FINE-TUNING\")\n",
    "\n",
    "transformer_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "\n",
    "def tokenize_batch_hf(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings_hf = tokenize_batch_hf(X_train_text, tokenizer_hf)\n",
    "val_encodings_hf   = tokenize_batch_hf(X_val_text,   tokenizer_hf)\n",
    "test_encodings_hf  = tokenize_batch_hf(X_test_text,  tokenizer_hf)\n",
    "\n",
    "class RumourEvalHFDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset_hf = RumourEvalHFDataset(train_encodings_hf, y_train_idx)\n",
    "val_dataset_hf   = RumourEvalHFDataset(val_encodings_hf,   y_val_idx)\n",
    "test_dataset_hf  = RumourEvalHFDataset(test_encodings_hf,  y_test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo para Transformer:\", device)\n",
    "\n",
    "model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    transformer_model_name,\n",
    "    num_labels=num_classes\n",
    ").to(device)\n",
    "\n",
    "optimizer_hf = AdamW(model_hf.parameters(), lr=2e-5)\n",
    "\n",
    "train_loader_hf = DataLoader(train_dataset_hf, batch_size=16, shuffle=True)\n",
    "val_loader_hf   = DataLoader(val_dataset_hf,   batch_size=32, shuffle=False)\n",
    "test_loader_hf  = DataLoader(test_dataset_hf,  batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs_hf = 3\n",
    "best_val_acc_hf = 0.0\n",
    "best_state_dict_hf = None\n",
    "\n",
    "for epoch in range(1, num_epochs_hf + 1):\n",
    "    model_hf.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in train_loader_hf:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer_hf.zero_grad()\n",
    "        outputs = model_hf(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_hf.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"labels\"].size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct_train += (preds == batch[\"labels\"]).sum().item()\n",
    "        total_train += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_loss = total_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    model_hf.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader_hf:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_hf(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct_val += (preds == batch[\"labels\"]).sum().item()\n",
    "            total_val += batch[\"labels\"].size(0)\n",
    "\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"[Transformer] Época {epoch}/{num_epochs_hf} | \"\n",
    "          f\"Loss train = {train_loss:.4f} | Acc train = {train_acc:.4f} | Acc val = {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_hf:\n",
    "        best_val_acc_hf = val_acc\n",
    "        best_state_dict_hf = model_hf.state_dict()\n",
    "\n",
    "print(f\"\\nMejor accuracy de validación (Transformer) = {best_val_acc_hf:.4f}\")\n",
    "\n",
    "if best_state_dict_hf is not None:\n",
    "    model_hf.load_state_dict(best_state_dict_hf)\n",
    "\n",
    "model_hf.eval()\n",
    "all_preds_hf = []\n",
    "all_true_hf = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader_hf:\n",
    "        labels = batch[\"labels\"].numpy().copy()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model_hf(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        all_preds_hf.append(preds)\n",
    "        all_true_hf.append(labels)\n",
    "\n",
    "all_preds_hf = np.concatenate(all_preds_hf)\n",
    "all_true_hf  = np.concatenate(all_true_hf)\n",
    "\n",
    "acc_test_transformer = accuracy_score(all_true_hf, all_preds_hf)\n",
    "print(f\"\\nAccuracy en TEST (Transformer fine-tuned: {transformer_model_name}) = {acc_test_transformer:.4f}\")\n",
    "\n",
    "y_test_pred_labels_transformer = label_encoder.inverse_transform(all_preds_hf)\n",
    "y_test_true_labels = label_encoder.inverse_transform(all_true_hf)\n",
    "\n",
    "print(\"\\nClassification report (TEST) - Transformer fine-tuned:\")\n",
    "print(classification_report(y_test_true_labels, y_test_pred_labels_transformer, digits=4))\n",
    "\n",
    "\n",
    "print(\"\\n\\nRESUMEN FINAL - KNN\")\n",
    "print(f\"TF-IDF (KNN):        mejor k = {best_k_tfidf},  accuracy test = {acc_test_tfidf:.4f}\")\n",
    "print(f\"Word2Vec (KNN):      mejor k = {best_k_w2v},    accuracy test = {acc_test_w2v:.4f}\")\n",
    "print(f\"Sentence-BERT (KNN): mejor k = {best_k_bert},   accuracy test = {acc_test_bert:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - CNN\")\n",
    "print(f\"TF-IDF  + CNN:        accuracy test = {acc_test_tfidf_cnn:.4f}\")\n",
    "print(f\"Word2Vec + CNN:       accuracy test = {acc_test_w2v_cnn:.4f}\")\n",
    "print(f\"Sentence-BERT + CNN:  accuracy test = {acc_test_bert_cnn:.4f}\")\n",
    "print(f\"\\nBaseline mayoría ('{major_class}') en TEST: accuracy = {baseline_acc:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - TRANSFORMER FINE-TUNED\")\n",
    "print(f\"Transformer ({transformer_model_name}): accuracy test = {acc_test_transformer:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumourenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
