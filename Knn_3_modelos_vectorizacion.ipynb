{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en label (antes de limpiar):\n",
      "  train: 2\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "NaN en label (después de limpiar):\n",
      "  train: 0\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "Etiquetas únicas en train: ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Distribución de clases:\n",
      "\n",
      "=== TRAIN (total = 4877) ===\n",
      "comment : 3495 (0.717)\n",
      "support :  642 (0.132)\n",
      "query   :  373 (0.076)\n",
      "deny    :  367 (0.075)\n",
      "\n",
      "=== VAL (total = 1440) ===\n",
      "comment : 1174 (0.815)\n",
      "query   :  114 (0.079)\n",
      "deny    :   79 (0.055)\n",
      "support :   73 (0.051)\n",
      "\n",
      "=== TEST (total = 1675) ===\n",
      "comment : 1405 (0.839)\n",
      "support :  104 (0.062)\n",
      "deny    :  100 (0.060)\n",
      "query   :   66 (0.039)\n",
      "\n",
      "Ejemplo de texto de entrenamiento:\n",
      "France: 10 people dead after shooting at HQ of satirical weekly newspaper #CharlieHebdo, according to witnesses http://t.co/FkYxGmuS58 [SEP] MT @euronews France: 10 dead after shooting at HQ of satirical weekly #CharlieHebdo. If Zionists/Jews did this they'd be nuking Israel\n",
      "Etiqueta: comment\n",
      "\n",
      "Clases (label_encoder): ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Clase mayoritaria en TEST: comment\n",
      "Accuracy baseline (siempre 'comment') = 0.8388\n",
      "\n",
      "\n",
      "EXPERIMENTO 1: TF-IDF + KNN\n",
      "\n",
      "============================================================\n",
      "RESULTADOS KNN - TF-IDF\n",
      "============================================================\n",
      "k = 1 --> Accuracy validación = 0.6528\n",
      "k = 3 --> Accuracy validación = 0.7153\n",
      "k = 5 --> Accuracy validación = 0.7403\n",
      "k = 7 --> Accuracy validación = 0.7542\n",
      "k = 9 --> Accuracy validación = 0.7910\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.7910\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8239\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8436    0.9786    0.9061      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.1220    0.0481    0.0690       104\n",
      "\n",
      "    accuracy                         0.8239      1675\n",
      "   macro avg     0.2414    0.2567    0.2438      1675\n",
      "weighted avg     0.7152    0.8239    0.7643      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'support' 'comment' 'comment' 'comment' 'support' 'comment'\n",
      " 'comment' 'comment' 'comment' 'support' 'support' 'support' 'comment'\n",
      " 'comment' 'comment' 'support' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\n",
      "Texto 0:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 1:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 2:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 3:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 4:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 5:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 6:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 7:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 8:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 9:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "\n",
      "\n",
      "EXPERIMENTO 1b: TF-IDF + CNN (PyTorch, normalizado + class weights)\n",
      "\n",
      "============================================================\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - TF-IDF + CNN (mejorada)\n",
      "============================================================\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n",
      "Época 01/15 | Loss train = 4.6798 | Acc train = 0.2737 | Acc val = 0.8160\n",
      "Época 02/15 | Loss train = 2.4003 | Acc train = 0.2678 | Acc val = 0.8153\n",
      "Época 03/15 | Loss train = 2.0020 | Acc train = 0.2540 | Acc val = 0.5417\n",
      "Época 04/15 | Loss train = 1.6736 | Acc train = 0.2536 | Acc val = 0.7785\n",
      "Época 05/15 | Loss train = 1.5390 | Acc train = 0.2397 | Acc val = 0.8146\n",
      "Época 06/15 | Loss train = 1.4491 | Acc train = 0.2491 | Acc val = 0.7882\n",
      "Época 07/15 | Loss train = 1.4337 | Acc train = 0.2573 | Acc val = 0.2014\n",
      "Época 08/15 | Loss train = 1.3982 | Acc train = 0.2912 | Acc val = 0.2736\n",
      "Época 09/15 | Loss train = 1.4007 | Acc train = 0.2930 | Acc val = 0.8139\n",
      "Época 10/15 | Loss train = 1.3955 | Acc train = 0.3430 | Acc val = 0.3160\n",
      "Época 11/15 | Loss train = 1.3913 | Acc train = 0.3268 | Acc val = 0.0826\n",
      "Época 12/15 | Loss train = 1.3942 | Acc train = 0.3410 | Acc val = 0.0563\n",
      "Época 13/15 | Loss train = 1.3893 | Acc train = 0.3283 | Acc val = 0.7535\n",
      "Época 14/15 | Loss train = 1.3969 | Acc train = 0.2596 | Acc val = 0.6583\n",
      "Época 15/15 | Loss train = 1.3936 | Acc train = 0.3527 | Acc val = 0.8146\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.8160\n",
      "\n",
      "Accuracy en TEST (TF-IDF + CNN (mejorada)) = 0.8388\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8388    1.0000    0.9123      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8388      1675\n",
      "   macro avg     0.2097    0.2500    0.2281      1675\n",
      "weighted avg     0.7036    0.8388    0.7653      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN TF-IDF + CNN (primeras 10 líneas)\n",
      "Texto 0:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 1:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 2:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 3:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 4:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 5:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 6:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 7:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 8:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 9:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "\n",
      "\n",
      "EXPERIMENTO 2: Word2Vec + KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS KNN - Word2Vec (media embeddings)\n",
      "============================================================\n",
      "k = 1 --> Accuracy validación = 0.6806\n",
      "k = 3 --> Accuracy validación = 0.7715\n",
      "k = 5 --> Accuracy validación = 0.8028\n",
      "k = 7 --> Accuracy validación = 0.8104\n",
      "k = 9 --> Accuracy validación = 0.8139\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.8139\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8388\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8392    0.9993    0.9123      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.5000    0.0096    0.0189       104\n",
      "\n",
      "    accuracy                         0.8388      1675\n",
      "   macro avg     0.3348    0.2522    0.2328      1675\n",
      "weighted avg     0.7350    0.8388    0.7664      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN Word2Vec + KNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "EXPERIMENTO 2b: Word2Vec + CNN (PyTorch, normalizado + class weights)\n",
      "\n",
      "============================================================\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - Word2Vec + CNN (mejorada)\n",
      "============================================================\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 01/30 | Loss train = 1.6196 | Acc train = 0.3195 | Acc val = 0.4854\n",
      "Época 02/30 | Loss train = 1.4485 | Acc train = 0.3119 | Acc val = 0.2965\n",
      "Época 03/30 | Loss train = 1.4104 | Acc train = 0.3213 | Acc val = 0.7000\n",
      "Época 04/30 | Loss train = 1.3625 | Acc train = 0.3250 | Acc val = 0.2819\n",
      "Época 05/30 | Loss train = 1.3381 | Acc train = 0.3453 | Acc val = 0.6458\n",
      "Época 06/30 | Loss train = 1.3383 | Acc train = 0.3742 | Acc val = 0.6479\n",
      "Época 07/30 | Loss train = 1.3267 | Acc train = 0.3531 | Acc val = 0.2604\n",
      "Época 08/30 | Loss train = 1.3104 | Acc train = 0.3662 | Acc val = 0.4611\n",
      "Época 09/30 | Loss train = 1.3033 | Acc train = 0.3514 | Acc val = 0.4444\n",
      "Época 10/30 | Loss train = 1.3058 | Acc train = 0.3668 | Acc val = 0.5708\n",
      "Época 11/30 | Loss train = 1.2869 | Acc train = 0.3799 | Acc val = 0.7118\n",
      "Época 12/30 | Loss train = 1.2762 | Acc train = 0.3631 | Acc val = 0.6438\n",
      "Época 13/30 | Loss train = 1.2823 | Acc train = 0.3810 | Acc val = 0.6979\n",
      "Época 14/30 | Loss train = 1.2567 | Acc train = 0.3836 | Acc val = 0.5569\n",
      "Época 15/30 | Loss train = 1.2755 | Acc train = 0.3664 | Acc val = 0.5312\n",
      "Época 16/30 | Loss train = 1.2371 | Acc train = 0.3758 | Acc val = 0.7618\n",
      "Época 17/30 | Loss train = 1.2664 | Acc train = 0.3683 | Acc val = 0.6875\n",
      "Época 18/30 | Loss train = 1.2357 | Acc train = 0.3896 | Acc val = 0.2944\n",
      "Época 19/30 | Loss train = 1.2504 | Acc train = 0.3869 | Acc val = 0.7576\n",
      "Época 20/30 | Loss train = 1.2336 | Acc train = 0.3857 | Acc val = 0.6090\n",
      "Época 21/30 | Loss train = 1.2113 | Acc train = 0.3781 | Acc val = 0.7014\n",
      "Época 22/30 | Loss train = 1.2058 | Acc train = 0.3972 | Acc val = 0.6986\n",
      "Época 23/30 | Loss train = 1.2099 | Acc train = 0.3966 | Acc val = 0.2646\n",
      "Época 24/30 | Loss train = 1.2026 | Acc train = 0.3875 | Acc val = 0.4271\n",
      "Época 25/30 | Loss train = 1.1827 | Acc train = 0.3988 | Acc val = 0.5590\n",
      "Época 26/30 | Loss train = 1.1763 | Acc train = 0.4132 | Acc val = 0.5625\n",
      "Época 27/30 | Loss train = 1.1811 | Acc train = 0.4138 | Acc val = 0.6715\n",
      "Época 28/30 | Loss train = 1.1686 | Acc train = 0.4156 | Acc val = 0.6701\n",
      "Época 29/30 | Loss train = 1.1626 | Acc train = 0.4177 | Acc val = 0.4319\n",
      "Época 30/30 | Loss train = 1.1363 | Acc train = 0.4123 | Acc val = 0.7014\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.7618\n",
      "\n",
      "Accuracy en TEST (Word2Vec + CNN (mejorada)) = 0.7415\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8484    0.8804    0.8641      1405\n",
      "        deny     0.0400    0.0100    0.0160       100\n",
      "       query     0.0209    0.0606    0.0311        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.7415      1675\n",
      "   macro avg     0.2273    0.2378    0.2278      1675\n",
      "weighted avg     0.7149    0.7415    0.7270      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'query' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN Word2Vec + CNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "EXPERIMENTO 3: EMBEDDINGS (Sentence-BERT) + KNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 153/153 [00:01<00:00, 96.97it/s] \n",
      "Batches: 100%|██████████| 45/45 [00:00<00:00, 123.42it/s]\n",
      "Batches: 100%|██████████| 53/53 [00:00<00:00, 118.59it/s]\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTADOS KNN - Embeddings contextuales (Sentence-BERT)\n",
      "============================================================\n",
      "k = 1 --> Accuracy validación = 0.6465\n",
      "k = 3 --> Accuracy validación = 0.6931\n",
      "k = 5 --> Accuracy validación = 0.7701\n",
      "k = 7 --> Accuracy validación = 0.7868\n",
      "k = 9 --> Accuracy validación = 0.8035\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.8035\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8304\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8395    0.9900    0.9086      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8304      1675\n",
      "   macro avg     0.2099    0.2475    0.2271      1675\n",
      "weighted avg     0.7042    0.8304    0.7621      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN BERT + KNN (primeras 10 líneas)\n",
      "0) pred=comment  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=comment  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=comment  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "EXPERIMENTO 3b: BERT Embeddings + CNN (PyTorch, normalizado + class weights)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - Sentence-BERT + CNN (mejorada)\n",
      "============================================================\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n",
      "Época 01/30 | Loss train = 1.6951 | Acc train = 0.2520 | Acc val = 0.6111\n",
      "Época 02/30 | Loss train = 1.5550 | Acc train = 0.2696 | Acc val = 0.7576\n",
      "Época 03/30 | Loss train = 1.4227 | Acc train = 0.3168 | Acc val = 0.7771\n",
      "Época 04/30 | Loss train = 1.4195 | Acc train = 0.2840 | Acc val = 0.8153\n",
      "Época 05/30 | Loss train = 1.4009 | Acc train = 0.3100 | Acc val = 0.6188\n",
      "Época 06/30 | Loss train = 1.3958 | Acc train = 0.3047 | Acc val = 0.5194\n",
      "Época 07/30 | Loss train = 1.3827 | Acc train = 0.3117 | Acc val = 0.6347\n",
      "Época 08/30 | Loss train = 1.3962 | Acc train = 0.3240 | Acc val = 0.1118\n",
      "Época 09/30 | Loss train = 1.3688 | Acc train = 0.3258 | Acc val = 0.0674\n",
      "Época 10/30 | Loss train = 1.3714 | Acc train = 0.3238 | Acc val = 0.7257\n",
      "Época 11/30 | Loss train = 1.3614 | Acc train = 0.3270 | Acc val = 0.7993\n",
      "Época 12/30 | Loss train = 1.3688 | Acc train = 0.3209 | Acc val = 0.6104\n",
      "Época 13/30 | Loss train = 1.3521 | Acc train = 0.3363 | Acc val = 0.6090\n",
      "Época 14/30 | Loss train = 1.3448 | Acc train = 0.3437 | Acc val = 0.5979\n",
      "Época 15/30 | Loss train = 1.3424 | Acc train = 0.3219 | Acc val = 0.7819\n",
      "Época 16/30 | Loss train = 1.3374 | Acc train = 0.3295 | Acc val = 0.7632\n",
      "Época 17/30 | Loss train = 1.3287 | Acc train = 0.3484 | Acc val = 0.1806\n",
      "Época 18/30 | Loss train = 1.3280 | Acc train = 0.3221 | Acc val = 0.5646\n",
      "Época 19/30 | Loss train = 1.3093 | Acc train = 0.3350 | Acc val = 0.8007\n",
      "Época 20/30 | Loss train = 1.3122 | Acc train = 0.3529 | Acc val = 0.4319\n",
      "Época 21/30 | Loss train = 1.2978 | Acc train = 0.3340 | Acc val = 0.4014\n",
      "Época 22/30 | Loss train = 1.2856 | Acc train = 0.3430 | Acc val = 0.6361\n",
      "Época 23/30 | Loss train = 1.2878 | Acc train = 0.3393 | Acc val = 0.4222\n",
      "Época 24/30 | Loss train = 1.2607 | Acc train = 0.3607 | Acc val = 0.5681\n",
      "Época 25/30 | Loss train = 1.2740 | Acc train = 0.3504 | Acc val = 0.4549\n",
      "Época 26/30 | Loss train = 1.2354 | Acc train = 0.3621 | Acc val = 0.4611\n",
      "Época 27/30 | Loss train = 1.2389 | Acc train = 0.3627 | Acc val = 0.6139\n",
      "Época 28/30 | Loss train = 1.1995 | Acc train = 0.3769 | Acc val = 0.7215\n",
      "Época 29/30 | Loss train = 1.2189 | Acc train = 0.3724 | Acc val = 0.3153\n",
      "Época 30/30 | Loss train = 1.2028 | Acc train = 0.3685 | Acc val = 0.6965\n",
      "\n",
      "Mejor accuracy de validación alcanzado: 0.8153\n",
      "\n",
      "Accuracy en TEST (Sentence-BERT + CNN (mejorada)) = 0.7707\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8378    0.9117    0.8732      1405\n",
      "        deny     1.0000    0.0200    0.0392       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.0576    0.0769    0.0658       104\n",
      "\n",
      "    accuracy                         0.7707      1675\n",
      "   macro avg     0.4738    0.2522    0.2446      1675\n",
      "weighted avg     0.7660    0.7707    0.7389      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['support' 'comment' 'comment' 'comment' 'support' 'comment' 'support'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n",
      "\n",
      "PREDICCIÓN BERT + CNN (primeras 10 líneas)\n",
      "0) pred=support  real=comment\n",
      "1) pred=comment  real=comment\n",
      "2) pred=comment  real=comment\n",
      "3) pred=comment  real=comment\n",
      "4) pred=support  real=comment\n",
      "5) pred=comment  real=comment\n",
      "6) pred=support  real=comment\n",
      "7) pred=comment  real=comment\n",
      "8) pred=comment  real=comment\n",
      "9) pred=comment  real=comment\n",
      "\n",
      "\n",
      "=== TRANSFORMER PREENTRENADO + FINE-TUNING (HUGGINGFACE, SIN TRAINER) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo para Transformer: cuda\n",
      "[Transformer] Época 1/3 | Loss train = 0.8365 | Acc train = 0.7205 | Acc val = 0.8313\n",
      "[Transformer] Época 2/3 | Loss train = 0.7163 | Acc train = 0.7542 | Acc val = 0.8208\n",
      "[Transformer] Época 3/3 | Loss train = 0.5818 | Acc train = 0.7989 | Acc val = 0.8028\n",
      "\n",
      "Mejor accuracy de validación (Transformer) = 0.8313\n",
      "\n",
      "Accuracy en TEST (Transformer fine-tuned: distilbert-base-uncased) = 0.8281\n",
      "\n",
      "Classification report (TEST) - Transformer fine-tuned:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8589    0.9530    0.9035      1405\n",
      "        deny     0.4571    0.1600    0.2370       100\n",
      "       query     0.3951    0.4848    0.4354        66\n",
      "     support     0.0000    0.0000    0.0000       104\n",
      "\n",
      "    accuracy                         0.8281      1675\n",
      "   macro avg     0.4278    0.3995    0.3940      1675\n",
      "weighted avg     0.7633    0.8281    0.7892      1675\n",
      "\n",
      "\n",
      "\n",
      "RESUMEN FINAL - KNN\n",
      "TF-IDF (KNN):        mejor k = 9,  accuracy test = 0.8239\n",
      "Word2Vec (KNN):      mejor k = 9,    accuracy test = 0.8388\n",
      "Sentence-BERT (KNN): mejor k = 9,   accuracy test = 0.8304\n",
      "\n",
      "RESUMEN FINAL - CNN (con normalización + class weights)\n",
      "TF-IDF  + CNN:        accuracy test = 0.8388\n",
      "Word2Vec + CNN:       accuracy test = 0.7415\n",
      "Sentence-BERT + CNN:  accuracy test = 0.7707\n",
      "\n",
      "Baseline mayoría ('comment') en TEST: accuracy = 0.8388\n",
      "\n",
      "RESUMEN FINAL - TRANSFORMER FINE-TUNED\n",
      "Transformer (distilbert-base-uncased): accuracy test = 0.8281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "train_path = \"data/datasets/rumoureval2019_train.csv\"\n",
    "val_path   = \"data/datasets/rumoureval2019_val.csv\"\n",
    "test_path  = \"data/datasets/rumoureval2019_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df   = pd.read_csv(val_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"NaN en label (antes de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"label\"])\n",
    "val_df   = val_df.dropna(subset=[\"label\"])\n",
    "test_df  = test_df.dropna(subset=[\"label\"])\n",
    "\n",
    "print(\"\\nNaN en label (después de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "print(\"\\nEtiquetas únicas en train:\", train_df[\"label\"].unique())\n",
    "\n",
    "print(\"\\nDistribución de clases:\")\n",
    "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    counts = df[\"label\"].value_counts()\n",
    "    total = counts.sum()\n",
    "    print(f\"\\n=== {name.upper()} (total = {total}) ===\")\n",
    "    for label, c in counts.items():\n",
    "        print(f\"{label:8s}: {c:4d} ({c/total:.3f})\")\n",
    "\n",
    "def concat_text_row(row):\n",
    "    src = row.get(\"source_text\", \"\")\n",
    "    rep = row.get(\"reply_text\", \"\")\n",
    "    src = \"\" if pd.isna(src) else str(src)\n",
    "    rep = \"\" if pd.isna(rep) else str(rep)\n",
    "    return (src + \" [SEP] \" + rep).strip()\n",
    "\n",
    "X_train_text = train_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_train = train_df[\"label\"].values         \n",
    "\n",
    "X_val_text = val_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "X_test_text = test_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\"\\nEjemplo de texto de entrenamiento:\")\n",
    "print(X_train_text[0])\n",
    "print(\"Etiqueta:\", y_train[0])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_idx = label_encoder.fit_transform(y_train)\n",
    "y_val_idx   = label_encoder.transform(y_val)\n",
    "y_test_idx  = label_encoder.transform(y_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nClases (label_encoder):\", label_encoder.classes_)\n",
    "\n",
    "major_class = Counter(y_test).most_common(1)[0][0]\n",
    "baseline_acc = np.mean(y_test == major_class)\n",
    "print(f\"\\nClase mayoritaria en TEST: {major_class}\")\n",
    "print(f\"Accuracy baseline (siempre '{major_class}') = {baseline_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate_knn(X_train_vec, y_train,\n",
    "                           X_val_vec, y_val,\n",
    "                           X_test_vec, y_test,\n",
    "                           k_values=[1, 3, 5, 7, 9],\n",
    "                           title=\"\"):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTADOS KNN -\", title)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    best_k = None\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_vec, y_train)\n",
    "        y_val_pred = knn.predict(X_val_vec)\n",
    "        acc_val = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"k = {k} --> Accuracy validación = {acc_val:.4f}\")\n",
    "\n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            best_k = k\n",
    "\n",
    "    print(\"\\nMejor número de vecinos (k) encontrado en validación:\", best_k)\n",
    "    print(f\"Accuracy de validación con k={best_k}: {best_acc:.4f}\")\n",
    "\n",
    "    final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    final_knn.fit(X_train_vec, y_train)\n",
    "\n",
    "    y_test_pred = final_knn.predict(X_test_vec)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"\\nAccuracy en TEST con k={best_k}: {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred[:20])\n",
    "    print(\"y_test[:20]      =\", y_test[:20])\n",
    "\n",
    "    return final_knn, best_k, acc_test\n",
    "\n",
    "\n",
    "class ConvNet1D(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, dropout=0.3):\n",
    "        super(ConvNet1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,   out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn4   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)           # (B, 1, L)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # (B, 64, L)\n",
    "\n",
    "        x = self.global_pool(x)      # (B, 64, 1)\n",
    "        x = x.squeeze(-1)            # (B, 64)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)               # (B, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_cnn(\n",
    "    X_train, y_train_idx,\n",
    "    X_val, y_val_idx,\n",
    "    X_test, y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"CNN\",\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3,\n",
    "    device=None\n",
    "):\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTRENANDO RED NEURONAL CONVOLUCIONAL -\", title)\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    X_val   = np.asarray(X_val,   dtype=np.float32)\n",
    "    X_test  = np.asarray(X_test,  dtype=np.float32)\n",
    "\n",
    "    y_train_idx = np.asarray(y_train_idx, dtype=np.int64)\n",
    "    y_val_idx   = np.asarray(y_val_idx,   dtype=np.int64)\n",
    "    y_test_idx  = np.asarray(y_test_idx,  dtype=np.int64)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_idx))\n",
    "    val_dataset   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val_idx))\n",
    "    test_dataset  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    class_weights_np = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.arange(num_classes),\n",
    "        y=y_train_idx\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
    "    print(\"\\nPesos de clase (para CrossEntropyLoss):\")\n",
    "    for idx, w in enumerate(class_weights_np):\n",
    "        print(f\"  Clase {idx} ({label_encoder.classes_[idx]}): {w:.4f}\")\n",
    "\n",
    "    model = ConvNet1D(input_dim=input_dim, num_classes=num_classes, dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == batch_y).sum().item()\n",
    "            total_train += batch_X.size(0)\n",
    "\n",
    "        train_loss = running_loss / total_train\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_X.size(0)\n",
    "\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        print(f\"Época {epoch:02d}/{num_epochs} | \"\n",
    "              f\"Loss train = {train_loss:.4f} | \"\n",
    "              f\"Acc train = {train_acc:.4f} | \"\n",
    "              f\"Acc val = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "    print(f\"\\nMejor accuracy de validación alcanzado: {best_val_acc:.4f}\")\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.inference_mode():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(batch_y.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_true  = np.concatenate(all_true)\n",
    "\n",
    "    y_test_pred_labels = label_encoder.inverse_transform(all_preds)\n",
    "    y_test_true_labels = label_encoder.inverse_transform(all_true)\n",
    "\n",
    "    acc_test = accuracy_score(y_test_true_labels, y_test_pred_labels)\n",
    "    print(f\"\\nAccuracy en TEST ({title}) = {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test_true_labels, y_test_pred_labels, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred_labels[:20])\n",
    "    print(\"y_test[:20]      =\", y_test_true_labels[:20])\n",
    "\n",
    "    return model, acc_test, y_test_pred_labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + KNN\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_val_tfidf   = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "knn_tfidf, best_k_tfidf, acc_test_tfidf = train_and_evaluate_knn(\n",
    "    X_train_tfidf, y_train,\n",
    "    X_val_tfidf, y_val,\n",
    "    X_test_tfidf, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"TF-IDF\"\n",
    ")\n",
    "\n",
    "y_pred_test = knn_tfidf.predict(X_test_tfidf)\n",
    "print(\"\\nPREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + CNN\")\n",
    "\n",
    "# Normalización con StandardScaler (with_mean=False porque es sparse)\n",
    "scaler_tfidf = StandardScaler(with_mean=False)\n",
    "X_train_tfidf_scaled = scaler_tfidf.fit_transform(X_train_tfidf)\n",
    "X_val_tfidf_scaled   = scaler_tfidf.transform(X_val_tfidf)\n",
    "X_test_tfidf_scaled  = scaler_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "# Pasamos a denso para PyTorch\n",
    "X_train_tfidf_dense = X_train_tfidf_scaled.toarray()\n",
    "X_val_tfidf_dense   = X_val_tfidf_scaled.toarray()\n",
    "X_test_tfidf_dense  = X_test_tfidf_scaled.toarray()\n",
    "\n",
    "cnn_tfidf, acc_test_tfidf_cnn, y_pred_test_tfidf_cnn = train_and_evaluate_cnn(\n",
    "    X_train_tfidf_dense, y_train_idx,\n",
    "    X_val_tfidf_dense,   y_val_idx,\n",
    "    X_test_tfidf_dense,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"TF-IDF + CNN\",\n",
    "    num_epochs=15,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN TF-IDF + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test_tfidf_cnn[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + KNN\")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "train_tokens = [simple_tokenize(t) for t in X_train_text]\n",
    "val_tokens   = [simple_tokenize(t) for t in X_val_text]\n",
    "test_tokens  = [simple_tokenize(t) for t in X_test_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "def document_embedding(tokens, word_vectors, dim=100):\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        if tok in word_vectors:\n",
    "            vecs.append(word_vectors[tok])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    else:\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "def build_doc_matrix(list_of_tokens, word_vectors, dim=100):\n",
    "    return np.vstack([\n",
    "        document_embedding(toks, word_vectors, dim)\n",
    "        for toks in list_of_tokens\n",
    "    ])\n",
    "\n",
    "X_train_w2v = build_doc_matrix(train_tokens, word_vectors, dim=100)\n",
    "X_val_w2v   = build_doc_matrix(val_tokens,   word_vectors, dim=100)\n",
    "X_test_w2v  = build_doc_matrix(test_tokens,  word_vectors, dim=100)\n",
    "\n",
    "knn_w2v, best_k_w2v, acc_test_w2v = train_and_evaluate_knn(\n",
    "    X_train_w2v, y_train,\n",
    "    X_val_w2v, y_val,\n",
    "    X_test_w2v, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Word2Vec (media embeddings)\"\n",
    ")\n",
    "\n",
    "y_pred_test_w2v = knn_w2v.predict(X_test_w2v)\n",
    "print(\"\\nPREDICCIÓN Word2Vec + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + CNN\")\n",
    "\n",
    "scaler_w2v = StandardScaler()\n",
    "X_train_w2v_scaled = scaler_w2v.fit_transform(X_train_w2v)\n",
    "X_val_w2v_scaled   = scaler_w2v.transform(X_val_w2v)\n",
    "X_test_w2v_scaled  = scaler_w2v.transform(X_test_w2v)\n",
    "\n",
    "cnn_w2v, acc_test_w2v_cnn, y_pred_test_w2v_cnn = train_and_evaluate_cnn(\n",
    "    X_train_w2v_scaled, y_train_idx,\n",
    "    X_val_w2v_scaled,   y_val_idx,\n",
    "    X_test_w2v_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Word2Vec + CNN\",\n",
    "    num_epochs=30,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN Word2Vec + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nEMBEDDINGS (Sentence-BERT) + KNN\")\n",
    "\n",
    "bert_model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "X_train_bert = bert_model_st.encode(X_train_text, batch_size=32, show_progress_bar=True)\n",
    "X_val_bert   = bert_model_st.encode(X_val_text,   batch_size=32, show_progress_bar=True)\n",
    "X_test_bert  = bert_model_st.encode(X_test_text,  batch_size=32, show_progress_bar=True)\n",
    "\n",
    "knn_bert, best_k_bert, acc_test_bert = train_and_evaluate_knn(\n",
    "    X_train_bert, y_train,\n",
    "    X_val_bert,   y_val,\n",
    "    X_test_bert,  y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Embeddings contextuales (Sentence-BERT)\"\n",
    ")\n",
    "\n",
    "y_pred_test_bert = knn_bert.predict(X_test_bert)\n",
    "print(\"\\nPREDICCIÓN BERT + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert[i]}  real={y_test[i]}\")\n",
    "\n",
    "print(\"\\n\\nBERT Embeddings + CNN \")\n",
    "\n",
    "scaler_bert = StandardScaler()\n",
    "X_train_bert_scaled = scaler_bert.fit_transform(X_train_bert)\n",
    "X_val_bert_scaled   = scaler_bert.transform(X_val_bert)\n",
    "X_test_bert_scaled  = scaler_bert.transform(X_test_bert)\n",
    "\n",
    "cnn_bert, acc_test_bert_cnn, y_pred_test_bert_cnn = train_and_evaluate_cnn(\n",
    "    X_train_bert_scaled, y_train_idx,\n",
    "    X_val_bert_scaled,   y_val_idx,\n",
    "    X_test_bert_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Sentence-BERT + CNN\",\n",
    "    num_epochs=30,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN BERT + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nTRANSFORMER PREENTRENADO + FINE-TUNING\")\n",
    "\n",
    "transformer_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "\n",
    "def tokenize_batch_hf(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings_hf = tokenize_batch_hf(X_train_text, tokenizer_hf)\n",
    "val_encodings_hf   = tokenize_batch_hf(X_val_text,   tokenizer_hf)\n",
    "test_encodings_hf  = tokenize_batch_hf(X_test_text,  tokenizer_hf)\n",
    "\n",
    "class RumourEvalHFDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset_hf = RumourEvalHFDataset(train_encodings_hf, y_train_idx)\n",
    "val_dataset_hf   = RumourEvalHFDataset(val_encodings_hf,   y_val_idx)\n",
    "test_dataset_hf  = RumourEvalHFDataset(test_encodings_hf,  y_test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo para Transformer:\", device)\n",
    "\n",
    "model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    transformer_model_name,\n",
    "    num_labels=num_classes\n",
    ").to(device)\n",
    "\n",
    "optimizer_hf = AdamW(model_hf.parameters(), lr=2e-5)\n",
    "\n",
    "train_loader_hf = DataLoader(train_dataset_hf, batch_size=16, shuffle=True)\n",
    "val_loader_hf   = DataLoader(val_dataset_hf,   batch_size=32, shuffle=False)\n",
    "test_loader_hf  = DataLoader(test_dataset_hf,  batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs_hf = 3\n",
    "best_val_acc_hf = 0.0\n",
    "best_state_dict_hf = None\n",
    "\n",
    "for epoch in range(1, num_epochs_hf + 1):\n",
    "    model_hf.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in train_loader_hf:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer_hf.zero_grad()\n",
    "        outputs = model_hf(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_hf.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"labels\"].size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct_train += (preds == batch[\"labels\"]).sum().item()\n",
    "        total_train += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_loss = total_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    model_hf.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader_hf:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_hf(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct_val += (preds == batch[\"labels\"]).sum().item()\n",
    "            total_val += batch[\"labels\"].size(0)\n",
    "\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"[Transformer] Época {epoch}/{num_epochs_hf} | \"\n",
    "          f\"Loss train = {train_loss:.4f} | Acc train = {train_acc:.4f} | Acc val = {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_hf:\n",
    "        best_val_acc_hf = val_acc\n",
    "        best_state_dict_hf = model_hf.state_dict()\n",
    "\n",
    "print(f\"\\nMejor accuracy de validación (Transformer) = {best_val_acc_hf:.4f}\")\n",
    "\n",
    "if best_state_dict_hf is not None:\n",
    "    model_hf.load_state_dict(best_state_dict_hf)\n",
    "\n",
    "model_hf.eval()\n",
    "all_preds_hf = []\n",
    "all_true_hf = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader_hf:\n",
    "        labels = batch[\"labels\"].numpy().copy()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model_hf(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        all_preds_hf.append(preds)\n",
    "        all_true_hf.append(labels)\n",
    "\n",
    "all_preds_hf = np.concatenate(all_preds_hf)\n",
    "all_true_hf  = np.concatenate(all_true_hf)\n",
    "\n",
    "acc_test_transformer = accuracy_score(all_true_hf, all_preds_hf)\n",
    "print(f\"\\nAccuracy en TEST (Transformer fine-tuned: {transformer_model_name}) = {acc_test_transformer:.4f}\")\n",
    "\n",
    "y_test_pred_labels_transformer = label_encoder.inverse_transform(all_preds_hf)\n",
    "y_test_true_labels = label_encoder.inverse_transform(all_true_hf)\n",
    "\n",
    "print(\"\\nClassification report (TEST) - Transformer fine-tuned:\")\n",
    "print(classification_report(y_test_true_labels, y_test_pred_labels_transformer, digits=4))\n",
    "\n",
    "\n",
    "print(\"\\n\\nRESUMEN FINAL - KNN\")\n",
    "print(f\"TF-IDF (KNN):        mejor k = {best_k_tfidf},  accuracy test = {acc_test_tfidf:.4f}\")\n",
    "print(f\"Word2Vec (KNN):      mejor k = {best_k_w2v},    accuracy test = {acc_test_w2v:.4f}\")\n",
    "print(f\"Sentence-BERT (KNN): mejor k = {best_k_bert},   accuracy test = {acc_test_bert:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - CNN\")\n",
    "print(f\"TF-IDF  + CNN:        accuracy test = {acc_test_tfidf_cnn:.4f}\")\n",
    "print(f\"Word2Vec + CNN:       accuracy test = {acc_test_w2v_cnn:.4f}\")\n",
    "print(f\"Sentence-BERT + CNN:  accuracy test = {acc_test_bert_cnn:.4f}\")\n",
    "print(f\"\\nBaseline mayoría ('{major_class}') en TEST: accuracy = {baseline_acc:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - TRANSFORMER FINE-TUNED\")\n",
    "print(f\"Transformer ({transformer_model_name}): accuracy test = {acc_test_transformer:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumourenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
