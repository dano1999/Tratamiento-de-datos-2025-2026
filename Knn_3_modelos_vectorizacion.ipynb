{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ae06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN en label (antes de limpiar):\n",
      "  train: 2\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "NaN en label (después de limpiar):\n",
      "  train: 0\n",
      "  val:   0\n",
      "  test:  0\n",
      "\n",
      "Etiquetas únicas en train: ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Distribución de clases:\n",
      "\n",
      " TRAIN (total = 4877) \n",
      "comment : 3495 (0.717)\n",
      "support :  642 (0.132)\n",
      "query   :  373 (0.076)\n",
      "deny    :  367 (0.075)\n",
      "\n",
      " VAL (total = 1440) \n",
      "comment : 1174 (0.815)\n",
      "query   :  114 (0.079)\n",
      "deny    :   79 (0.055)\n",
      "support :   73 (0.051)\n",
      "\n",
      " TEST (total = 1675) \n",
      "comment : 1405 (0.839)\n",
      "support :  104 (0.062)\n",
      "deny    :  100 (0.060)\n",
      "query   :   66 (0.039)\n",
      "\n",
      "Ejemplo de texto de entrenamiento:\n",
      "France: 10 people dead after shooting at HQ of satirical weekly newspaper #CharlieHebdo, according to witnesses http://t.co/FkYxGmuS58 [SEP] MT @euronews France: 10 dead after shooting at HQ of satirical weekly #CharlieHebdo. If Zionists/Jews did this they'd be nuking Israel\n",
      "Etiqueta: comment\n",
      "\n",
      "Clases (label_encoder): ['comment' 'deny' 'query' 'support']\n",
      "\n",
      "Clase mayoritaria en TEST: comment\n",
      "Accuracy baseline (siempre 'comment') = 0.8388\n",
      "\n",
      "\n",
      "TF-IDF + KNN\n",
      "RESULTADOS KNN - TF-IDF\n",
      "k = 1 --> Accuracy validación = 0.6632\n",
      "k = 3 --> Accuracy validación = 0.7118\n",
      "k = 5 --> Accuracy validación = 0.7389\n",
      "k = 7 --> Accuracy validación = 0.7681\n",
      "k = 9 --> Accuracy validación = 0.7903\n",
      "\n",
      "Mejor número de vecinos (k) encontrado en validación: 9\n",
      "Accuracy de validación con k=9: 0.7903\n",
      "\n",
      "Accuracy en TEST con k=9: 0.8299\n",
      "\n",
      "Classification report (TEST):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     comment     0.8412    0.9879    0.9087      1405\n",
      "        deny     0.0000    0.0000    0.0000       100\n",
      "       query     0.0000    0.0000    0.0000        66\n",
      "     support     0.1000    0.0192    0.0323       104\n",
      "\n",
      "    accuracy                         0.8299      1675\n",
      "   macro avg     0.2353    0.2518    0.2352      1675\n",
      "weighted avg     0.7118    0.8299    0.7642      1675\n",
      "\n",
      "\n",
      "Ejemplo de predicciones en test (primeros 20):\n",
      "y_test_pred[:20] = ['comment' 'support' 'comment' 'comment' 'comment' 'support' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'support' 'support' 'comment'\n",
      " 'comment' 'comment' 'support' 'comment' 'comment' 'comment']\n",
      "y_test[:20]      = ['comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'comment' 'comment' 'comment' 'comment' 'comment'\n",
      " 'comment' 'comment' 'query' 'comment' 'comment' 'query']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Pablo\\OneDrive\\Desktop\\PROYECTO TRATAMIENTO DE DATOS\\Tratamiento-de-datos-2025-2026\\rumourenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\n",
      "Texto 0:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 1:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 2:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 3:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 4:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 5:\n",
      "   Predicción: support\n",
      "   Real:       comment\n",
      "Texto 6:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 7:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 8:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "Texto 9:\n",
      "   Predicción: comment\n",
      "   Real:       comment\n",
      "\n",
      "\n",
      "TF-IDF + CNN\n",
      "ENTRENANDO RED NEURONAL CONVOLUCIONAL - TF-IDF + CNN\n",
      "Usando dispositivo: cuda\n",
      "\n",
      "Pesos de clase (para CrossEntropyLoss):\n",
      "  Clase 0 (comment): 0.3489\n",
      "  Clase 1 (deny): 3.3222\n",
      "  Clase 2 (query): 3.2688\n",
      "  Clase 3 (support): 1.8991\n",
      "Época 01/15 | Loss train = 4.7329 | Acc train = 0.2538 | Acc val = 0.7549\n",
      "Época 02/15 | Loss train = 2.5998 | Acc train = 0.2700 | Acc val = 0.0549\n",
      "Época 03/15 | Loss train = 2.1093 | Acc train = 0.2741 | Acc val = 0.0715\n",
      "Época 04/15 | Loss train = 1.7290 | Acc train = 0.2737 | Acc val = 0.8111\n",
      "Época 05/15 | Loss train = 1.5367 | Acc train = 0.2815 | Acc val = 0.7549\n",
      "Época 06/15 | Loss train = 1.4590 | Acc train = 0.2723 | Acc val = 0.8146\n",
      "Época 07/15 | Loss train = 1.4342 | Acc train = 0.2932 | Acc val = 0.8125\n",
      "Época 08/15 | Loss train = 1.4202 | Acc train = 0.2860 | Acc val = 0.7667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_path = \"data/datasets/rumoureval2019_train.csv\"\n",
    "val_path   = \"data/datasets/rumoureval2019_val.csv\"\n",
    "test_path  = \"data/datasets/rumoureval2019_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df   = pd.read_csv(val_path)\n",
    "test_df  = pd.read_csv(test_path)\n",
    "\n",
    "print(\"NaN en label (antes de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "train_df = train_df.dropna(subset=[\"label\"])\n",
    "val_df   = val_df.dropna(subset=[\"label\"])\n",
    "test_df  = test_df.dropna(subset=[\"label\"])\n",
    "\n",
    "print(\"\\nNaN en label (después de limpiar):\")\n",
    "print(\"  train:\", train_df[\"label\"].isna().sum())\n",
    "print(\"  val:  \", val_df[\"label\"].isna().sum())\n",
    "print(\"  test: \", test_df[\"label\"].isna().sum())\n",
    "\n",
    "print(\"\\nEtiquetas únicas en train:\", train_df[\"label\"].unique())\n",
    "\n",
    "print(\"\\nDistribución de clases:\")\n",
    "for name, df in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    counts = df[\"label\"].value_counts()\n",
    "    total = counts.sum()\n",
    "    print(f\"\\n {name.upper()} (total = {total}) \")\n",
    "    for label, c in counts.items():\n",
    "        print(f\"{label:8s}: {c:4d} ({c/total:.3f})\")\n",
    "\n",
    "def concat_text_row(row):\n",
    "    src = row.get(\"source_text\", \"\")\n",
    "    rep = row.get(\"reply_text\", \"\")\n",
    "    src = \"\" if pd.isna(src) else str(src)\n",
    "    rep = \"\" if pd.isna(rep) else str(rep)\n",
    "    return (src + \" [SEP] \" + rep).strip()\n",
    "\n",
    "X_train_text = train_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_train = train_df[\"label\"].values         \n",
    "\n",
    "X_val_text = val_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_val = val_df[\"label\"].values\n",
    "\n",
    "X_test_text = test_df.apply(concat_text_row, axis=1).tolist()\n",
    "y_test = test_df[\"label\"].values\n",
    "\n",
    "print(\"\\nEjemplo de texto de entrenamiento:\")\n",
    "print(X_train_text[0])\n",
    "print(\"Etiqueta:\", y_train[0])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_idx = label_encoder.fit_transform(y_train)\n",
    "y_val_idx   = label_encoder.transform(y_val)\n",
    "y_test_idx  = label_encoder.transform(y_test)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nClases (label_encoder):\", label_encoder.classes_)\n",
    "\n",
    "major_class = Counter(y_test).most_common(1)[0][0]\n",
    "baseline_acc = np.mean(y_test == major_class)\n",
    "print(f\"\\nClase mayoritaria en TEST: {major_class}\")\n",
    "print(f\"Accuracy baseline (siempre '{major_class}') = {baseline_acc:.4f}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate_knn(X_train_vec, y_train,\n",
    "                           X_val_vec, y_val,\n",
    "                           X_test_vec, y_test,\n",
    "                           k_values=[1, 3, 5, 7, 9],\n",
    "                           title=\"\"):\n",
    "    print(\"RESULTADOS KNN -\", title)\n",
    "\n",
    "    best_k = None\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for k in k_values:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train_vec, y_train)\n",
    "        y_val_pred = knn.predict(X_val_vec)\n",
    "        acc_val = accuracy_score(y_val, y_val_pred)\n",
    "        print(f\"k = {k} --> Accuracy validación = {acc_val:.4f}\")\n",
    "\n",
    "        if acc_val > best_acc:\n",
    "            best_acc = acc_val\n",
    "            best_k = k\n",
    "\n",
    "    print(\"\\nMejor número de vecinos (k) encontrado en validación:\", best_k)\n",
    "    print(f\"Accuracy de validación con k={best_k}: {best_acc:.4f}\")\n",
    "\n",
    "    final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "    final_knn.fit(X_train_vec, y_train)\n",
    "\n",
    "    y_test_pred = final_knn.predict(X_test_vec)\n",
    "    acc_test = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    print(f\"\\nAccuracy en TEST con k={best_k}: {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test, y_test_pred, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred[:20])\n",
    "    print(\"y_test[:20]      =\", y_test[:20])\n",
    "\n",
    "    return final_knn, best_k, acc_test\n",
    "\n",
    "\n",
    "class ConvNet1D(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, dropout=0.3):\n",
    "        super(ConvNet1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=1,   out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn2   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn3   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(in_channels=64,  out_channels=64, kernel_size=5, padding=2)\n",
    "        self.bn4   = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)           # (B, 1, L)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # (B, 64, L)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))  # (B, 64, L)\n",
    "\n",
    "        x = self.global_pool(x)      # (B, 64, 1)\n",
    "        x = x.squeeze(-1)            # (B, 64)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)               # (B, num_classes)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_cnn(\n",
    "    X_train, y_train_idx,\n",
    "    X_val, y_val_idx,\n",
    "    X_test, y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"CNN\",\n",
    "    num_epochs=20,\n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3,\n",
    "    device=None\n",
    "):\n",
    "\n",
    "    print(\"ENTRENANDO RED NEURONAL CONVOLUCIONAL -\", title)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Usando dispositivo:\", device)\n",
    "\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    X_val   = np.asarray(X_val,   dtype=np.float32)\n",
    "    X_test  = np.asarray(X_test,  dtype=np.float32)\n",
    "\n",
    "    y_train_idx = np.asarray(y_train_idx, dtype=np.int64)\n",
    "    y_val_idx   = np.asarray(y_val_idx,   dtype=np.int64)\n",
    "    y_test_idx  = np.asarray(y_test_idx,  dtype=np.int64)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_idx))\n",
    "    val_dataset   = TensorDataset(torch.from_numpy(X_val),   torch.from_numpy(y_val_idx))\n",
    "    test_dataset  = TensorDataset(torch.from_numpy(X_test),  torch.from_numpy(y_test_idx))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    class_weights_np = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.arange(num_classes),\n",
    "        y=y_train_idx\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
    "    print(\"\\nPesos de clase (para CrossEntropyLoss):\")\n",
    "    for idx, w in enumerate(class_weights_np):\n",
    "        print(f\"  Clase {idx} ({label_encoder.classes_[idx]}): {w:.4f}\")\n",
    "\n",
    "    model = ConvNet1D(input_dim=input_dim, num_classes=num_classes, dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = None\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_X.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += (preds == batch_y).sum().item()\n",
    "            total_train += batch_X.size(0)\n",
    "\n",
    "        train_loss = running_loss / total_train\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.inference_mode():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == batch_y).sum().item()\n",
    "                total_val += batch_X.size(0)\n",
    "\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        print(f\"Época {epoch:02d}/{num_epochs} | \"\n",
    "              f\"Loss train = {train_loss:.4f} | \"\n",
    "              f\"Acc train = {train_acc:.4f} | \"\n",
    "              f\"Acc val = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = model.state_dict()\n",
    "\n",
    "    print(f\"\\nMejor accuracy de validación alcanzado: {best_val_acc:.4f}\")\n",
    "\n",
    "    if best_state_dict is not None:\n",
    "        model.load_state_dict(best_state_dict)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    with torch.inference_mode():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_true.append(batch_y.numpy())\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_true  = np.concatenate(all_true)\n",
    "\n",
    "    y_test_pred_labels = label_encoder.inverse_transform(all_preds)\n",
    "    y_test_true_labels = label_encoder.inverse_transform(all_true)\n",
    "\n",
    "    acc_test = accuracy_score(y_test_true_labels, y_test_pred_labels)\n",
    "    print(f\"\\nAccuracy en TEST ({title}) = {acc_test:.4f}\")\n",
    "    print(\"\\nClassification report (TEST):\")\n",
    "    print(classification_report(y_test_true_labels, y_test_pred_labels, digits=4))\n",
    "\n",
    "    print(\"\\nEjemplo de predicciones en test (primeros 20):\")\n",
    "    print(\"y_test_pred[:20] =\", y_test_pred_labels[:20])\n",
    "    print(\"y_test[:20]      =\", y_test_true_labels[:20])\n",
    "\n",
    "    return model, acc_test, y_test_pred_labels\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + KNN\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_val_tfidf   = tfidf_vectorizer.transform(X_val_text)\n",
    "X_test_tfidf  = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "knn_tfidf, best_k_tfidf, acc_test_tfidf = train_and_evaluate_knn(\n",
    "    X_train_tfidf, y_train,\n",
    "    X_val_tfidf, y_val,\n",
    "    X_test_tfidf, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"TF-IDF\"\n",
    ")\n",
    "\n",
    "y_pred_test = knn_tfidf.predict(X_test_tfidf)\n",
    "print(\"\\nPREDICCIÓN TF-IDF + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nTF-IDF + CNN\")\n",
    "\n",
    "scaler_tfidf = StandardScaler(with_mean=False)\n",
    "X_train_tfidf_scaled = scaler_tfidf.fit_transform(X_train_tfidf)\n",
    "X_val_tfidf_scaled   = scaler_tfidf.transform(X_val_tfidf)\n",
    "X_test_tfidf_scaled  = scaler_tfidf.transform(X_test_tfidf)\n",
    "\n",
    "X_train_tfidf_dense = X_train_tfidf_scaled.toarray()\n",
    "X_val_tfidf_dense   = X_val_tfidf_scaled.toarray()\n",
    "X_test_tfidf_dense  = X_test_tfidf_scaled.toarray()\n",
    "\n",
    "cnn_tfidf, acc_test_tfidf_cnn, y_pred_test_tfidf_cnn = train_and_evaluate_cnn(\n",
    "    X_train_tfidf_dense, y_train_idx,\n",
    "    X_val_tfidf_dense,   y_val_idx,\n",
    "    X_test_tfidf_dense,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"TF-IDF + CNN\",\n",
    "    num_epochs=15,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN TF-IDF + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"Texto {i}:\")\n",
    "    print(\"   Predicción:\", y_pred_test_tfidf_cnn[i])\n",
    "    print(\"   Real:      \", y_test[i])\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + KNN\")\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return str(text).lower().split()\n",
    "\n",
    "train_tokens = [simple_tokenize(t) for t in X_train_text]\n",
    "val_tokens   = [simple_tokenize(t) for t in X_val_text]\n",
    "test_tokens  = [simple_tokenize(t) for t in X_test_text]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=train_tokens,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1\n",
    ")\n",
    "\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "def document_embedding(tokens, word_vectors, dim=100):\n",
    "    vecs = []\n",
    "    for tok in tokens:\n",
    "        if tok in word_vectors:\n",
    "            vecs.append(word_vectors[tok])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    else:\n",
    "        return np.mean(vecs, axis=0)\n",
    "\n",
    "def build_doc_matrix(list_of_tokens, word_vectors, dim=100):\n",
    "    return np.vstack([\n",
    "        document_embedding(toks, word_vectors, dim)\n",
    "        for toks in list_of_tokens\n",
    "    ])\n",
    "\n",
    "X_train_w2v = build_doc_matrix(train_tokens, word_vectors, dim=100)\n",
    "X_val_w2v   = build_doc_matrix(val_tokens,   word_vectors, dim=100)\n",
    "X_test_w2v  = build_doc_matrix(test_tokens,  word_vectors, dim=100)\n",
    "\n",
    "knn_w2v, best_k_w2v, acc_test_w2v = train_and_evaluate_knn(\n",
    "    X_train_w2v, y_train,\n",
    "    X_val_w2v, y_val,\n",
    "    X_test_w2v, y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Word2Vec (media embeddings)\"\n",
    ")\n",
    "\n",
    "y_pred_test_w2v = knn_w2v.predict(X_test_w2v)\n",
    "print(\"\\nPREDICCIÓN Word2Vec + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nWord2Vec + CNN\")\n",
    "\n",
    "scaler_w2v = StandardScaler()\n",
    "X_train_w2v_scaled = scaler_w2v.fit_transform(X_train_w2v)\n",
    "X_val_w2v_scaled   = scaler_w2v.transform(X_val_w2v)\n",
    "X_test_w2v_scaled  = scaler_w2v.transform(X_test_w2v)\n",
    "\n",
    "cnn_w2v, acc_test_w2v_cnn, y_pred_test_w2v_cnn = train_and_evaluate_cnn(\n",
    "    X_train_w2v_scaled, y_train_idx,\n",
    "    X_val_w2v_scaled,   y_val_idx,\n",
    "    X_test_w2v_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Word2Vec + CNN (mejorada)\",\n",
    "    num_epochs=35,       \n",
    "    batch_size=32,\n",
    "    lr=3e-4,             \n",
    "    dropout=0.4\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN Word2Vec + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_w2v_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nEMBEDDINGS (Sentence-BERT) + KNN\")\n",
    "\n",
    "bert_model_st = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "X_train_bert = bert_model_st.encode(X_train_text, batch_size=32, show_progress_bar=True)\n",
    "X_val_bert   = bert_model_st.encode(X_val_text,   batch_size=32, show_progress_bar=True)\n",
    "X_test_bert  = bert_model_st.encode(X_test_text,  batch_size=32, show_progress_bar=True)\n",
    "\n",
    "knn_bert, best_k_bert, acc_test_bert = train_and_evaluate_knn(\n",
    "    X_train_bert, y_train,\n",
    "    X_val_bert,   y_val,\n",
    "    X_test_bert,  y_test,\n",
    "    k_values=[1, 3, 5, 7, 9],\n",
    "    title=\"Embeddings contextuales (Sentence-BERT)\"\n",
    ")\n",
    "\n",
    "y_pred_test_bert = knn_bert.predict(X_test_bert)\n",
    "print(\"\\nPREDICCIÓN BERT + KNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert[i]}  real={y_test[i]}\")\n",
    "\n",
    "print(\"\\n\\nBERT Embeddings + CNN \")\n",
    "\n",
    "scaler_bert = StandardScaler()\n",
    "X_train_bert_scaled = scaler_bert.fit_transform(X_train_bert)\n",
    "X_val_bert_scaled   = scaler_bert.transform(X_val_bert)\n",
    "X_test_bert_scaled  = scaler_bert.transform(X_test_bert)\n",
    "\n",
    "cnn_bert, acc_test_bert_cnn, y_pred_test_bert_cnn = train_and_evaluate_cnn(\n",
    "    X_train_bert_scaled, y_train_idx,\n",
    "    X_val_bert_scaled,   y_val_idx,\n",
    "    X_test_bert_scaled,  y_test_idx,\n",
    "    label_encoder,\n",
    "    title=\"Sentence-BERT + CNN\",\n",
    "    num_epochs=30,       \n",
    "    batch_size=32,\n",
    "    lr=5e-4,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(\"\\nPREDICCIÓN BERT + CNN (primeras 10 líneas)\")\n",
    "for i in range(10):\n",
    "    print(f\"{i}) pred={y_pred_test_bert_cnn[i]}  real={y_test[i]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nTRANSFORMER PREENTRENADO + FINE-TUNING\")\n",
    "\n",
    "transformer_model_name = \"distilbert-base-uncased\"\n",
    "tokenizer_hf = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "\n",
    "def tokenize_batch_hf(texts, tokenizer, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings_hf = tokenize_batch_hf(X_train_text, tokenizer_hf)\n",
    "val_encodings_hf   = tokenize_batch_hf(X_val_text,   tokenizer_hf)\n",
    "test_encodings_hf  = tokenize_batch_hf(X_test_text,  tokenizer_hf)\n",
    "\n",
    "class RumourEvalHFDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset_hf = RumourEvalHFDataset(train_encodings_hf, y_train_idx)\n",
    "val_dataset_hf   = RumourEvalHFDataset(val_encodings_hf,   y_val_idx)\n",
    "test_dataset_hf  = RumourEvalHFDataset(test_encodings_hf,  y_test_idx)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Dispositivo para Transformer:\", device)\n",
    "\n",
    "model_hf = AutoModelForSequenceClassification.from_pretrained(\n",
    "    transformer_model_name,\n",
    "    num_labels=num_classes\n",
    ").to(device)\n",
    "\n",
    "optimizer_hf = AdamW(model_hf.parameters(), lr=2e-5)\n",
    "\n",
    "train_loader_hf = DataLoader(train_dataset_hf, batch_size=16, shuffle=True)\n",
    "val_loader_hf   = DataLoader(val_dataset_hf,   batch_size=32, shuffle=False)\n",
    "test_loader_hf  = DataLoader(test_dataset_hf,  batch_size=32, shuffle=False)\n",
    "\n",
    "num_epochs_hf = 3\n",
    "best_val_acc_hf = 0.0\n",
    "best_state_dict_hf = None\n",
    "\n",
    "for epoch in range(1, num_epochs_hf + 1):\n",
    "    model_hf.train()\n",
    "    total_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for batch in train_loader_hf:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        optimizer_hf.zero_grad()\n",
    "        outputs = model_hf(**batch)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer_hf.step()\n",
    "\n",
    "        total_loss += loss.item() * batch[\"labels\"].size(0)\n",
    "        preds = logits.argmax(dim=-1)\n",
    "        correct_train += (preds == batch[\"labels\"]).sum().item()\n",
    "        total_train += batch[\"labels\"].size(0)\n",
    "\n",
    "    train_loss = total_loss / total_train\n",
    "    train_acc = correct_train / total_train\n",
    "\n",
    "    model_hf.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader_hf:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model_hf(**batch)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct_val += (preds == batch[\"labels\"]).sum().item()\n",
    "            total_val += batch[\"labels\"].size(0)\n",
    "\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"[Transformer] Época {epoch}/{num_epochs_hf} | \"\n",
    "          f\"Loss train = {train_loss:.4f} | Acc train = {train_acc:.4f} | Acc val = {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_hf:\n",
    "        best_val_acc_hf = val_acc\n",
    "        best_state_dict_hf = model_hf.state_dict()\n",
    "\n",
    "print(f\"\\nMejor accuracy de validación (Transformer) = {best_val_acc_hf:.4f}\")\n",
    "\n",
    "if best_state_dict_hf is not None:\n",
    "    model_hf.load_state_dict(best_state_dict_hf)\n",
    "\n",
    "model_hf.eval()\n",
    "all_preds_hf = []\n",
    "all_true_hf = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in test_loader_hf:\n",
    "        labels = batch[\"labels\"].numpy().copy()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model_hf(**batch)\n",
    "        logits = outputs.logits\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        all_preds_hf.append(preds)\n",
    "        all_true_hf.append(labels)\n",
    "\n",
    "all_preds_hf = np.concatenate(all_preds_hf)\n",
    "all_true_hf  = np.concatenate(all_true_hf)\n",
    "\n",
    "acc_test_transformer = accuracy_score(all_true_hf, all_preds_hf)\n",
    "print(f\"\\nAccuracy en TEST (Transformer fine-tuned: {transformer_model_name}) = {acc_test_transformer:.4f}\")\n",
    "\n",
    "y_test_pred_labels_transformer = label_encoder.inverse_transform(all_preds_hf)\n",
    "y_test_true_labels = label_encoder.inverse_transform(all_true_hf)\n",
    "\n",
    "print(\"\\nClassification report (TEST) - Transformer fine-tuned:\")\n",
    "print(classification_report(y_test_true_labels, y_test_pred_labels_transformer, digits=4))\n",
    "\n",
    "\n",
    "print(\"\\n\\nRESUMEN FINAL - KNN\")\n",
    "print(f\"TF-IDF (KNN):        mejor k = {best_k_tfidf},  accuracy test = {acc_test_tfidf:.4f}\")\n",
    "print(f\"Word2Vec (KNN):      mejor k = {best_k_w2v},    accuracy test = {acc_test_w2v:.4f}\")\n",
    "print(f\"Sentence-BERT (KNN): mejor k = {best_k_bert},   accuracy test = {acc_test_bert:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - CNN\")\n",
    "print(f\"TF-IDF  + CNN:        accuracy test = {acc_test_tfidf_cnn:.4f}\")\n",
    "print(f\"Word2Vec + CNN:       accuracy test = {acc_test_w2v_cnn:.4f}\")\n",
    "print(f\"Sentence-BERT + CNN:  accuracy test = {acc_test_bert_cnn:.4f}\")\n",
    "print(f\"\\nBaseline mayoría ('{major_class}') en TEST: accuracy = {baseline_acc:.4f}\")\n",
    "\n",
    "print(\"\\nRESUMEN FINAL - TRANSFORMER FINE-TUNED\")\n",
    "print(f\"Transformer ({transformer_model_name}): accuracy test = {acc_test_transformer:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\nBASELINE LÉXICA + URL (MEJORADA, DATA-DRIVEN)\")\n",
    "\n",
    "\n",
    "_url_re = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "\n",
    "def extract_urls(text):\n",
    "    return _url_re.findall(str(text))\n",
    "\n",
    "def extract_domain(url):\n",
    "    u = url.lower()\n",
    "    u = u.replace(\"http://\", \"\").replace(\"https://\", \"\")\n",
    "    if u.startswith(\"www.\"):\n",
    "        u = u[4:]\n",
    "    return u.split(\"/\")[0].strip()\n",
    "\n",
    "_token_re = re.compile(r\"[a-zA-Z']+\")\n",
    "\n",
    "def tokenize_basic(text):\n",
    "\n",
    "    return _token_re.findall(str(text).lower())\n",
    "\n",
    "def bigrams(tokens):\n",
    "    return [tokens[i] + \"_\" + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "\n",
    "\n",
    "\n",
    "labels = list(label_encoder.classes_)\n",
    "label_set = set(labels)\n",
    "\n",
    "\n",
    "tok_counts = {lab: Counter() for lab in labels}\n",
    "bi_counts  = {lab: Counter() for lab in labels}\n",
    "total_tok  = {lab: 0 for lab in labels}\n",
    "total_bi   = {lab: 0 for lab in labels}\n",
    "\n",
    "\n",
    "domain_counts = {lab: Counter() for lab in labels}\n",
    "\n",
    "for text, lab in zip(X_train_text, y_train):\n",
    "    tks = tokenize_basic(text)\n",
    "    tok_counts[lab].update(tks)\n",
    "    total_tok[lab] += len(tks)\n",
    "\n",
    "    bis = bigrams(tks)\n",
    "    bi_counts[lab].update(bis)\n",
    "    total_bi[lab] += len(bis)\n",
    "\n",
    "    for u in extract_urls(text):\n",
    "        dom = extract_domain(u)\n",
    "        if dom:\n",
    "            domain_counts[lab][dom] += 1\n",
    "\n",
    "\n",
    "vocab_tok = set()\n",
    "vocab_bi  = set()\n",
    "for lab in labels:\n",
    "    vocab_tok |= set(tok_counts[lab].keys())\n",
    "    vocab_bi  |= set(bi_counts[lab].keys())\n",
    "\n",
    "V_tok = len(vocab_tok) if len(vocab_tok) > 0 else 1\n",
    "V_bi  = len(vocab_bi)  if len(vocab_bi)  > 0 else 1\n",
    "\n",
    "\n",
    "alpha_tok = 0.5\n",
    "alpha_bi  = 0.5\n",
    "\n",
    "def build_log_odds_lexicon(counts_by_class, totals_by_class, V, top_k=120):\n",
    "\n",
    "    lexicon = {lab: {} for lab in labels}\n",
    "\n",
    "\n",
    "    total_all = sum(totals_by_class.values())\n",
    "\n",
    "    for lab in labels:\n",
    "        total_lab = totals_by_class[lab]\n",
    "        total_oth = total_all - total_lab\n",
    "\n",
    "\n",
    "        others = Counter()\n",
    "        for other_lab in labels:\n",
    "            if other_lab != lab:\n",
    "                others.update(counts_by_class[other_lab])\n",
    "\n",
    "        scored = []\n",
    "        for term in counts_by_class[lab].keys():\n",
    "            c_lab = counts_by_class[lab][term]\n",
    "            c_oth = others.get(term, 0)\n",
    "\n",
    "            p_lab = (c_lab + alpha_tok) / (total_lab + alpha_tok * V)\n",
    "            p_oth = (c_oth + alpha_tok) / (total_oth + alpha_tok * V)\n",
    "\n",
    "            score = np.log(p_lab) - np.log(p_oth)\n",
    "            scored.append((term, score))\n",
    "\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        for term, score in scored[:top_k]:\n",
    "            lexicon[lab][term] = score\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "lex_tok = build_log_odds_lexicon(tok_counts, total_tok, V_tok, top_k=200)\n",
    "lex_bi  = build_log_odds_lexicon(bi_counts,  total_bi,  V_bi,  top_k=120)\n",
    "\n",
    "\n",
    "min_dom_freq = 3\n",
    "domain_global = Counter()\n",
    "for lab in labels:\n",
    "    domain_global.update(domain_counts[lab])\n",
    "\n",
    "domains_kept = {d for d, c in domain_global.items() if c >= min_dom_freq}\n",
    "\n",
    "\n",
    "dom_scores = {lab: {} for lab in labels}\n",
    "alpha_dom = 0.5\n",
    "V_dom = len(domains_kept) if len(domains_kept) > 0 else 1\n",
    "\n",
    "total_dom = {lab: sum(domain_counts[lab][d] for d in domains_kept) for lab in labels}\n",
    "total_dom_all = sum(total_dom.values())\n",
    "\n",
    "for lab in labels:\n",
    "\n",
    "    others = Counter()\n",
    "    for other_lab in labels:\n",
    "        if other_lab != lab:\n",
    "            for d in domains_kept:\n",
    "                others[d] += domain_counts[other_lab][d]\n",
    "\n",
    "    total_lab = total_dom[lab]\n",
    "    total_oth = total_dom_all - total_lab\n",
    "\n",
    "    for d in domains_kept:\n",
    "        c_lab = domain_counts[lab][d]\n",
    "        c_oth = others[d]\n",
    "\n",
    "        p_lab = (c_lab + alpha_dom) / (total_lab + alpha_dom * V_dom)\n",
    "        p_oth = (c_oth + alpha_dom) / (total_oth + alpha_dom * V_dom)\n",
    "\n",
    "        dom_scores[lab][d] = float(np.log(p_lab) - np.log(p_oth))\n",
    "\n",
    "\n",
    "w_tok = 1.0\n",
    "w_bi  = 0.8\n",
    "w_dom = 1.2\n",
    "w_qmark = 0.8  \n",
    "\n",
    "def predict_lex_url(text, majority_label):\n",
    "    t = str(text)\n",
    "    t_low = t.lower()\n",
    "    tks = tokenize_basic(t_low)\n",
    "    bis = bigrams(tks)\n",
    "    urls = extract_urls(t_low)\n",
    "    doms = [extract_domain(u) for u in urls]\n",
    "    doms = [d for d in doms if d]\n",
    "\n",
    "    scores = {lab: 0.0 for lab in labels}\n",
    "\n",
    "\n",
    "    for tok in tks:\n",
    "        for lab in labels:\n",
    "            if tok in lex_tok[lab]:\n",
    "                scores[lab] += w_tok * lex_tok[lab][tok]\n",
    "\n",
    "\n",
    "    for bi in bis:\n",
    "        for lab in labels:\n",
    "            if bi in lex_bi[lab]:\n",
    "                scores[lab] += w_bi * lex_bi[lab][bi]\n",
    "\n",
    "\n",
    "    for d in doms:\n",
    "        if d in domains_kept:\n",
    "            for lab in labels:\n",
    "                scores[lab] += w_dom * dom_scores[lab].get(d, 0.0)\n",
    "\n",
    "\n",
    "    if \"query\" in label_set and \"?\" in t:\n",
    "        scores[\"query\"] += w_qmark\n",
    "\n",
    "\n",
    "    best_lab = max(scores.items(), key=lambda x: x[1])[0]\n",
    "    if abs(scores[best_lab]) < 1e-6:\n",
    "        return majority_label\n",
    "    return best_lab\n",
    "\n",
    "y_test_lex2 = np.array([predict_lex_url(txt, major_class) for txt in X_test_text])\n",
    "\n",
    "acc_lex2 = accuracy_score(y_test, y_test_lex2)\n",
    "print(f\"\\nAccuracy baseline léxica+URL (mejorada) = {acc_lex2:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (TEST) - Baseline léxica+URL (mejorada):\")\n",
    "print(classification_report(y_test, y_test_lex2, digits=4))\n",
    "\n",
    "print(\"\\nEjemplo de predicciones (primeros 20 ejemplos del TEST):\")\n",
    "for i in range(20):\n",
    "    print(f\"{i}) real={y_test[i]:8s}  pred={y_test_lex2[i]:8s}  text={X_test_text[i][:70]!r}...\")\n",
    "\n",
    "\n",
    "acc_hybrid=acc_lex2\n",
    "# TABLA RESUMEN DE ACCURACIES\n",
    "results = {\n",
    "    \"Baseline mayoría (siempre comment)\": baseline_acc,\n",
    "    \"Baseline híbrida (léxica + URL)\": acc_hybrid,\n",
    "    \"TF-IDF + KNN\": acc_test_tfidf,\n",
    "    \"Word2Vec + KNN\": acc_test_w2v,\n",
    "    \"SBERT + KNN\": acc_test_bert,\n",
    "    \"TF-IDF + CNN\": acc_test_tfidf_cnn,\n",
    "    \"Word2Vec + CNN\": acc_test_w2v_cnn,\n",
    "    \"SBERT + CNN\": acc_test_bert_cnn,\n",
    "    f\"Transformer fine-tuned ({transformer_model_name})\": acc_test_transformer,\n",
    "}\n",
    "\n",
    "df_results = (\n",
    "    pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"accuracy\"])\n",
    "      .sort_values(\"accuracy\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"RESUMEN DE ACCURACIES (ordenado de mayor a menor):\\n\")\n",
    "display(df_results)\n",
    "\n",
    "\n",
    "# GRÁFICO GLOBAL DE TODOS LOS MODELOS\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh(df_results.index, df_results[\"accuracy\"])\n",
    "plt.xlabel(\"Accuracy en test\")\n",
    "plt.title(\"Comparación global de modelos (accuracy)\")\n",
    "plt.xlim(0, 1.0)\n",
    "\n",
    "for i, v in enumerate(df_results[\"accuracy\"]):\n",
    "    plt.text(v + 0.01, i, f\"{v:.3f}\", va=\"center\")\n",
    "\n",
    "plt.gca().invert_yaxis()  \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "models_knn = {\n",
    "    \"TF-IDF + KNN\": acc_test_tfidf,\n",
    "    \"Word2Vec + KNN\": acc_test_w2v,\n",
    "    \"SBERT + KNN\": acc_test_bert,\n",
    "}\n",
    "\n",
    "df_knn = pd.DataFrame.from_dict(models_knn, orient=\"index\", columns=[\"accuracy\"])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(df_knn.index, df_knn[\"accuracy\"])\n",
    "plt.ylabel(\"Accuracy en test\")\n",
    "plt.title(\"Comparación KNN con distintas representaciones\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "for i, v in enumerate(df_knn[\"accuracy\"]):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "\n",
    "\n",
    "\n",
    "models_cnn = {\n",
    "    \"TF-IDF + CNN\": acc_test_tfidf_cnn,\n",
    "    \"Word2Vec + CNN\": acc_test_w2v_cnn,\n",
    "    \"SBERT + CNN\": acc_test_bert_cnn,\n",
    "    f\"Transformer fine-tuned ({transformer_model_name})\": acc_test_transformer,\n",
    "}\n",
    "\n",
    "df_cnn = pd.DataFrame.from_dict(models_cnn, orient=\"index\", columns=[\"accuracy\"])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(df_cnn.index, df_cnn[\"accuracy\"])\n",
    "plt.ylabel(\"Accuracy en test\")\n",
    "plt.title(\"Comparación CNN vs Transformer\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=20)\n",
    "\n",
    "for i, v in enumerate(df_cnn[\"accuracy\"]):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "models_baseline = {\n",
    "    \"Baseline mayoría\": baseline_acc,\n",
    "    \"Baseline híbrida\": acc_hybrid,\n",
    "    f\"Transformer fine-tuned ({transformer_model_name})\": acc_test_transformer,\n",
    "}\n",
    "\n",
    "df_base = pd.DataFrame.from_dict(models_baseline, orient=\"index\", columns=[\"accuracy\"])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(df_base.index, df_base[\"accuracy\"])\n",
    "plt.ylabel(\"Accuracy en test\")\n",
    "plt.title(\"Baselines simbólicas vs Transformer fine-tuned\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=20)\n",
    "\n",
    "for i, v in enumerate(df_base[\"accuracy\"]):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "label_counts = Counter(y_test)\n",
    "labels = list(label_counts.keys())\n",
    "counts = [label_counts[l] for l in labels]\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.bar(labels, counts)\n",
    "plt.title(\"Distribución de clases en el conjunto de test\")\n",
    "plt.ylabel(\"Número de ejemplos\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "rep_transf = classification_report(\n",
    "    y_test_true_labels,\n",
    "    y_test_pred_labels_transformer,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Baseline mayoría siempre 'comment'\n",
    "y_test_majority = np.array([major_class] * len(y_test))\n",
    "rep_majority = classification_report(\n",
    "    y_test,\n",
    "    y_test_majority,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "labels = label_encoder.classes_  # ['comment', 'deny', 'query', 'support']\n",
    "\n",
    "f1_transf = [rep_transf[l][\"f1-score\"] for l in labels]\n",
    "f1_major  = [rep_majority[l][\"f1-score\"] for l in labels]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(x - width/2, f1_major,  width, label=\"Baseline mayoría\")\n",
    "plt.bar(x + width/2, f1_transf, width, label=\"Transformer\")\n",
    "\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.title(\"F1 por clase: baseline vs Transformer\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Etiquetas en el orden correcto\n",
    "labels = label_encoder.classes_\n",
    "\n",
    "# Matriz de confusión del Transformer\n",
    "cm = confusion_matrix(y_test_true_labels,\n",
    "                      y_test_pred_labels_transformer,\n",
    "                      labels=labels)\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "ax = plt.gca()\n",
    "\n",
    "im = ax.imshow(cm)  # sin cmap para no complicar\n",
    "\n",
    "# Ejes\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "ax.set_title(\"Matriz de confusión – Transformer fine-tuned\")\n",
    "\n",
    "# Escribir los números dentro de cada celda\n",
    "for i in range(len(labels)):\n",
    "    for j in range(len(labels)):\n",
    "        ax.text(j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rumourenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
